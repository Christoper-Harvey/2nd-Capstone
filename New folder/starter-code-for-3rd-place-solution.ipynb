{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "testing = True #Set this to true for submission/False for cross validation\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "X_train = pd.merge(X_train,y_train,on='series_id')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X_train['surface'] = le.fit_transform(X_train['surface'])\n",
    "if(testing):\n",
    "    X_test = pd.read_csv('X_test.csv')\n",
    "    X_test['series_id'] = X_test['series_id']+3810\n",
    "    X_test['group_id'] = 0\n",
    "    X_test['surface'] = 0\n",
    "    frames = [X_train,X_test]\n",
    "    X_train = pd.concat(frames)\n",
    "    X_train.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "221072c02a371cac63d07e9592941c6e65bf8a42"
   },
   "outputs": [],
   "source": [
    "cols = list(X_train.columns.values)\n",
    "cols.remove('orientation_W')\n",
    "cols.insert(3,'orientation_W')\n",
    "X_train = X_train[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "71d24843e14f36b7c2b0860eea9b7d4c3e6f57a2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_meas = 128\n",
    "num_series = X_train['series_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "603241626d86bf9eeacb6ee9ed95d3ecec99e0fd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def q_to_angle(q_val):\n",
    "    #We assume q_val is in this format: [qw, q1, q2, q3]\n",
    "    #And the quaternion is normalized\n",
    "    roll = np.arctan2(2*(q_val[0]*q_val[1] + q_val[2]*q_val[3]),1 - 2*(q_val[1]*q_val[1] + q_val[2]*q_val[2]))\n",
    "    pitch = np.arcsin(2*(q_val[0]*q_val[2] - q_val[3]*q_val[1]))\n",
    "    yaw = np.arctan2(2*(q_val[0]*q_val[3] + q_val[1]*q_val[2]),1 - 2*(q_val[2]*q_val[2] + q_val[3]*q_val[3]))\n",
    "    return np.array([roll, pitch, yaw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "cf416b97ff135f37f7287bb331b5ac25b442bd71",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "quat_arr = np.array(X_train[['orientation_W','orientation_X','orientation_Y','orientation_Z']])\n",
    "euler_arr = np.zeros([quat_arr.shape[0],3])\n",
    "for n,arr in enumerate(quat_arr):\n",
    "    euler_arr[n] = q_to_angle(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "83569b323e21d295b47e5f0a8053b21b96dd0298",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train['roll'] = euler_arr[:,0]\n",
    "X_train['pitch'] = euler_arr[:,1]\n",
    "X_train['yaw'] = euler_arr[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "39e0baed88650cc85b720cdc098ba8288681ca44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = list(X_train.columns.values)\n",
    "cols.remove('group_id')\n",
    "cols.append('group_id')\n",
    "cols.remove('surface')\n",
    "cols.append('surface')\n",
    "X_train = X_train[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "6a61e694bf681474b358a0862004de4973b428d9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_cols = ['roll','pitch','yaw','angular_velocity_X','angular_velocity_Y','angular_velocity_Z','linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z']\n",
    "feat_array = np.array(X_train[feat_cols])\n",
    "feat_array = np.reshape(feat_array,[num_series,128,len(feat_cols)])\n",
    "group_array = np.array(X_train['group_id'])\n",
    "group_array = np.reshape(group_array,[num_series,128])\n",
    "group_array = group_array[:,0]\n",
    "target_array = np.array(X_train['surface'])\n",
    "target_array = np.reshape(target_array,[num_series,128])\n",
    "target_array = target_array[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "537abf133ddfbb65dd2c7e29f5ceee444facaeee",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use the first order difference of the following features\n",
    "#Absolute Orientation features dont make sense to predict surface\n",
    "delta_cols = ['roll','pitch','yaw']\n",
    "for dc in delta_cols:\n",
    "    iia = feat_cols.index(dc)\n",
    "    np_arr = feat_array[:,:,iia]\n",
    "    roll_arr = np.copy(np_arr)\n",
    "    roll_arr[:,1:] = roll_arr[:,:-1]\n",
    "    np_arr = np_arr - roll_arr\n",
    "    feat_array[:,:,iia] = np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "194f265bd2cba051f61b1b1d529e0ddf9b1c37c8"
   },
   "outputs": [],
   "source": [
    "#Normalize each 128-pt sample to ensure there is no group related information left in the samples\n",
    "norm_cols = ['linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z','angular_velocity_X','angular_velocity_Y','angular_velocity_Z']\n",
    "for norm in norm_cols:\n",
    "    iia = feat_cols.index(norm)\n",
    "    np_arr = feat_array[:,:,iia]\n",
    "    mean_arr = np.mean(np_arr,1)\n",
    "    mean_arr = np.expand_dims(mean_arr,1)\n",
    "    mean_arr = np.repeat(mean_arr,num_meas,1)\n",
    "    np_arr = np_arr - mean_arr\n",
    "    feat_array[:,:,iia] = np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "a31f546817b501b93a4855db477aa043c56c22d8"
   },
   "outputs": [],
   "source": [
    "def absfft(x):\n",
    "    return np.abs(np.fft.rfft(x))\n",
    "\n",
    "feat_fft_array = np.copy(feat_array[:,:,3:])\n",
    "feat_fft_array = np.apply_along_axis(absfft,1,feat_fft_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "4b0c58012bd2144fcac6bb9f515340ab57393f07"
   },
   "outputs": [],
   "source": [
    "#Further normalization across the entire dataset to ensure NN inputs are zero-mean and unit standard deviation\n",
    "\n",
    "num_sensor = feat_array.shape[2]\n",
    "for i in range(num_sensor):\n",
    "    mean_s = np.mean(feat_array[:,:,i])\n",
    "    sd_s = np.std(feat_array[:,:,i])\n",
    "    feat_array[:,:,i] = (feat_array[:,:,i]-mean_s)/sd_s\n",
    "\n",
    "num_sensor_fft = feat_fft_array.shape[2]\n",
    "for i in range(num_sensor_fft):\n",
    "    mean_s = np.mean(feat_fft_array[:,:,i])\n",
    "    sd_s = np.std(feat_fft_array[:,:,i])\n",
    "    feat_fft_array[:,:,i] = (feat_fft_array[:,:,i]-mean_s)/sd_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "495ca800870408b60542ee298087f4253726dad4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,Dense, Dropout, BatchNormalization, SeparableConv1D, Reshape, LSTM, DepthwiseConv2D,AveragePooling2D, CuDNNLSTM, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.backend import squeeze\n",
    "from keras.regularizers import l2\n",
    "kr = None\n",
    "num_groups = np.unique(group_array).shape[0]\n",
    "num_surfaces = np.unique(target_array).shape[0]\n",
    "\n",
    "def get_net_with_fft_mag_only(dp):\n",
    "    inputs_t = Input(shape=(128,len(feat_cols)))\n",
    "    x = SeparableConv1D(32,8,2,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(inputs_t)\n",
    "    x = Dropout(dp)(x)\n",
    "    x = SeparableConv1D(64,8,4,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(x)\n",
    "    x = Dropout(dp)(x)\n",
    "    x = SeparableConv1D(128,8,4,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(x)\n",
    "    x = Dropout(dp)(x)\n",
    "    x = SeparableConv1D(256,8,4,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(x)\n",
    "    x = Reshape((256,))(x)\n",
    "    x = Dropout(dp)(x)\n",
    "    x = Dense(64, activation='relu',kernel_regularizer=kr)(x)\n",
    "    x = Dropout(dp)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    \n",
    "    inputs_f = Input(shape=(feat_fft_array.shape[1],feat_fft_array.shape[2]))\n",
    "    y = SeparableConv1D(32,8,2,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(inputs_f)\n",
    "    y = Dropout(dp)(y)\n",
    "    y = SeparableConv1D(64,8,2,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(y)\n",
    "    y = Dropout(dp)(y)\n",
    "    y = SeparableConv1D(128,8,4,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(y)\n",
    "    y = Dropout(dp)(y)\n",
    "    y = SeparableConv1D(128,8,4,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(y)\n",
    "    y = Dropout(dp)(y)\n",
    "    y = SeparableConv1D(256,8,2,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(y)\n",
    "    y = Reshape((256,))(y)\n",
    "    y = Dropout(dp)(y)\n",
    "    y = Dense(64, activation='relu',kernel_regularizer=kr)(y)\n",
    "    y = Dropout(dp)(y)\n",
    "    y = Dense(64, activation='relu')(y)\n",
    "    \n",
    "        \n",
    "    inputs = [inputs_t,inputs_f]\n",
    "    \n",
    "    z = Concatenate()([x,y])\n",
    "    z = Dense(64, activation='relu')(z)\n",
    "    predictions = Dense(num_surfaces, activation='softmax')(z)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "e384f3e8a2cc567b2c2a4be08bd2d05f6fd9d127"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c5846d6242d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mnnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_net_with_fft_mag_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt_feats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_feats_fft\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mpred_classes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv_feats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv_feats_fft\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mpred_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "depthwise = False\n",
    "fft_net = True\n",
    "if(not(testing)):\n",
    "    gkf = GroupKFold(3)\n",
    "    train_gen = gkf.split(X=feat_array,groups=group_array)\n",
    "    preds = np.zeros_like(target_array)\n",
    "    for train_idx,test_idx in train_gen:\n",
    "        #Test features\n",
    "        t_feats = feat_array[train_idx]\n",
    "        t_feats_fft = feat_fft_array[train_idx]\n",
    "        \n",
    "        #Validation features\n",
    "        v_feats = feat_array[test_idx]\n",
    "        v_feats_fft = feat_fft_array[test_idx]\n",
    "        \n",
    "        t_vals = target_array[train_idx]\n",
    "        v_vals = target_array[test_idx]\n",
    "        \n",
    "        pred_classes = np.zeros([v_vals.shape[0],num_surfaces,5])\n",
    "        for k in range(5): #5 time averaging to get more stable results\n",
    "            nnet = get_net_with_fft_mag_only(0.5)\n",
    "            nnet.fit(x=[t_feats,t_feats_fft],y=t_vals,batch_size=256,epochs=3000,validation_data=([v_feats,v_feats_fft],v_vals),verbose=2)\n",
    "            pred_classes[:,:,k] = nnet.predict([v_feats,v_feats_fft])\n",
    "        pred_classes = np.mean(pred_classes,axis=2)\n",
    "        pred_classes = np.argmax(pred_classes,axis=1)\n",
    "        preds[test_idx] = pred_classes\n",
    "        print('Val accuracy: ',accuracy_score(v_vals,pred_classes))\n",
    "        pred_classes = nnet.predict([t_feats,t_feats_fft])\n",
    "        pred_classes = np.argmax(pred_classes,axis=1)\n",
    "        print('Train accuracy: ',accuracy_score(t_vals,pred_classes))\n",
    "    print('5 Fold accuracy: ', accuracy_score(target_array,preds))\n",
    "else:\n",
    "    t_feats = feat_array[:3810]\n",
    "    t_feats_fft = feat_fft_array[:3810]\n",
    "    t_vals = target_array[:3810]\n",
    "    v_feats = feat_array[3810:]\n",
    "    v_feats_fft = feat_fft_array[3810:]\n",
    "    pred_classes = np.zeros([v_feats.shape[0],num_surfaces,3])\n",
    "    for k in range(3):\n",
    "        nnet = get_net_with_fft_mag_only(0.5)\n",
    "        nnet.fit(x=[t_feats,t_feats_fft],y=t_vals,batch_size=256,epochs=3000,verbose=0)\n",
    "        pred_classes[:,:,k] = nnet.predict([v_feats,v_feats_fft])\n",
    "    pred_classes = np.mean(pred_classes,axis=2)\n",
    "    pred_classes = list(np.argmax(pred_classes,axis=1))\n",
    "    pred_classes = [le.inverse_transform([i])[0] for i in pred_classes]\n",
    "#     sub_df = pd.read_csv('../input/sample_submission.csv')\n",
    "#     sub_df['surface'] = pred_classes\n",
    "#     sub_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
