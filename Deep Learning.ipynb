{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages are ready!\n"
     ]
    }
   ],
   "source": [
    "# Lets first import our packages for everything. If you don't have a package installed you can use !pip install package_name\n",
    "\n",
    "# These are for automatic hyperparameter optimization\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "\n",
    "# Our standard packages for data science.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# These are for augmenting and finding the data. We won't be using SKLearn much for actually modelling.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# These are the actual packages for deep learning. We will mostly use the high level keras package for tensorflow.\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.optimizers import *\n",
    "from keras.layers import LeakyReLU, Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "print('Packages are ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "# Lets import our data\n",
    "target = pd.read_csv('target.csv')\n",
    "data = pd.read_csv('data.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "vdata = pd.read_csv('vdata.csv')\n",
    "vtarget = pd.read_csv('vtarget.csv')\n",
    "ltarget = pd.read_csv('ltarget.csv', header=None)\n",
    "lvtarget = pd.read_csv('lvtarget.csv', header=None)\n",
    "wdata = pd.read_csv('whole_data.csv')\n",
    "wtarget = pd.read_csv('whole_target.csv', header=None)\n",
    "\n",
    "winedata = pd.read_csv('winedata.csv')\n",
    "\n",
    "ccdata = pd.read_csv('creditcard.csv')\n",
    "\n",
    "irisdata = pd.read_csv('irisdata.csv')\n",
    "iristarget = pd.read_csv('iristarget.csv')\n",
    "\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets go ahead and set up our data. First lets make our target and drop it from wine.\n",
    "winetarget = winedata['quality']\n",
    "winedata = winedata.drop('quality', axis=1)\n",
    "\n",
    "# Lets trim off time and amount from ccdata as those are independent features we don't want the model to learn.\n",
    "cctarget = ccdata['Class']\n",
    "ccdata = ccdata.drop(['Time','Amount', 'Class'], axis=1)\n",
    "\n",
    "iristarget = iristarget['target']\n",
    "\n",
    "target = target['surface']\n",
    "vtarget = vtarget['surface']\n",
    "ltarget = ltarget[0]\n",
    "lvtarget = lvtarget[0]\n",
    "wdata = wdata.drop(['series_id', 'group_id', 'surface'], axis=1)\n",
    "wtarget = wtarget[0]\n",
    "\n",
    "data = data.values\n",
    "vdata = vdata.values\n",
    "wdata = wdata.values\n",
    "test = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "(6497, 11)\n",
      "(6497,)\n",
      "(284807, 28)\n",
      "(284807,)\n",
      "(2804,)\n",
      "(1006,)\n",
      "(358912,)\n",
      "(128768,)\n",
      "(487680,)\n",
      "(487680, 23)\n",
      "(128768, 23)\n",
      "(358912, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(488448, 23)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets make sure our train and target variables are even for every dataset\n",
    "print(irisdata.shape)\n",
    "print(iristarget.shape)\n",
    "\n",
    "print(winedata.shape)\n",
    "print(winetarget.shape)\n",
    "\n",
    "print(ccdata.shape)\n",
    "print(cctarget.shape)\n",
    "\n",
    "print(target.shape)\n",
    "print(vtarget.shape)\n",
    "print(ltarget.shape)\n",
    "print(lvtarget.shape)\n",
    "print(wtarget.shape)\n",
    "print(wdata.shape)\n",
    "print(vdata.shape)\n",
    "print(data.shape)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! All our data is set up properally and is ready to be worked on!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_dic = {0: 'fine_concrete',\n",
    "              1: 'concrete',\n",
    "              2: 'soft_tiles',\n",
    "              3: 'tiled',\n",
    "              4: 'soft_pvc',\n",
    "              5: 'hard_tiles_large_space',\n",
    "              6: 'carpet',\n",
    "              7: 'hard_tiles',\n",
    "              8: 'wood'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for changing quickly the data\n",
    "# data = data.reshape((2804, 128, 23))\n",
    "# vdata = vdata.reshape((1006, 128, 23))\n",
    "# test = test.reshape((3816, 128, 23))\n",
    "\n",
    "data = data.reshape((358912, 23))\n",
    "vdata = vdata.reshape((128768, 23))\n",
    "test = test.reshape((488448, 23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(optimizer, learning_rate, activation, dropout_rate,\n",
    "                initilizer,num_unit):\n",
    "    keras.backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_unit, kernel_initializer=initilizer,\n",
    "                    activation=activation, input_shape=(784,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_unit, kernel_initializer=initilizer,\n",
    "                    activation=activation))\n",
    "    model.add(Dropout(dropout_rate)) \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [:1] is for testing\n",
    "\n",
    "batch_size = [20, 50, 100][:1]\n",
    "\n",
    "epochs = [1, 20, 50][:1]\n",
    "\n",
    "initilizer = ['lecun_uniform', 'normal', 'he_normal', 'he_uniform'][:1]\n",
    "\n",
    "learning_rate = [0.1, 0.001, 0.02][:1]\n",
    "\n",
    "dropout_rate = [0.3, 0.2, 0.8][:1]\n",
    "\n",
    "num_unit = [10, 5][:1]\n",
    "\n",
    "activation = ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'][:1]\n",
    "\n",
    "optimizer = [SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat the wrapper and pass params to GridSearchCV \n",
    "# parameters is a dict with all values\n",
    "\n",
    "parameters = dict(batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  dropout_rate = dropout_rate,\n",
    "                  num_unit = num_unit,\n",
    "                  initilizer = initilizer,\n",
    "                  learning_rate = learning_rate,\n",
    "                  activation = activation,\n",
    "                  optimizer = optimizer)\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, verbose=0)\n",
    "\n",
    "models = GridSearchCV(estimator = model, param_grid=parameters, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model :\n",
      "{   'activation': 'relu',\n",
      "    'batch_size': 20,\n",
      "    'dropout_rate': 0.3,\n",
      "    'epochs': 1,\n",
      "    'initilizer': 'lecun_uniform',\n",
      "    'learning_rate': 0.1,\n",
      "    'num_unit': 10,\n",
      "    'optimizer': <class 'keras.optimizers.SGD'>}\n"
     ]
    }
   ],
   "source": [
    "best_model = models.fit(x_train, y_train)\n",
    "print('Best model :')\n",
    "pp.pprint(best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "num_classes = 9\n",
    "\n",
    "# Here is a standard keras fully connected model.\n",
    "model.add(Dense(64, input_dim=23, use_bias=False, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(64, use_bias=False, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(64, use_bias=False, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# loss='mearn_squared_error'\n",
    "# 'adam'\n",
    "# 'Nadam'\n",
    "# loss = kullback_leibler_divergence\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 358912 samples, validate on 128768 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229888/358912 [==================>...........] - ETA: 37:39 - loss: 2.7451 - acc: 0.13 - ETA: 4:59 - loss: 2.6580 - acc: 0.1514 - ETA: 2:29 - loss: 2.6677 - acc: 0.158 - ETA: 1:43 - loss: 2.6681 - acc: 0.155 - ETA: 1:20 - loss: 2.6506 - acc: 0.156 - ETA: 1:07 - loss: 2.6316 - acc: 0.158 - ETA: 59s - loss: 2.6240 - acc: 0.161 - ETA: 52s - loss: 2.6172 - acc: 0.16 - ETA: 48s - loss: 2.6116 - acc: 0.16 - ETA: 44s - loss: 2.6065 - acc: 0.16 - ETA: 41s - loss: 2.5953 - acc: 0.16 - ETA: 39s - loss: 2.5944 - acc: 0.16 - ETA: 37s - loss: 2.5894 - acc: 0.17 - ETA: 35s - loss: 2.5857 - acc: 0.17 - ETA: 34s - loss: 2.5798 - acc: 0.17 - ETA: 32s - loss: 2.5782 - acc: 0.17 - ETA: 31s - loss: 2.5730 - acc: 0.17 - ETA: 30s - loss: 2.5702 - acc: 0.17 - ETA: 29s - loss: 2.5691 - acc: 0.17 - ETA: 29s - loss: 2.5657 - acc: 0.17 - ETA: 28s - loss: 2.5619 - acc: 0.17 - ETA: 27s - loss: 2.5572 - acc: 0.17 - ETA: 26s - loss: 2.5523 - acc: 0.17 - ETA: 26s - loss: 2.5493 - acc: 0.17 - ETA: 25s - loss: 2.5457 - acc: 0.17 - ETA: 25s - loss: 2.5428 - acc: 0.17 - ETA: 24s - loss: 2.5402 - acc: 0.17 - ETA: 24s - loss: 2.5380 - acc: 0.18 - ETA: 24s - loss: 2.5372 - acc: 0.17 - ETA: 23s - loss: 2.5336 - acc: 0.18 - ETA: 23s - loss: 2.5301 - acc: 0.18 - ETA: 22s - loss: 2.5278 - acc: 0.18 - ETA: 22s - loss: 2.5256 - acc: 0.18 - ETA: 22s - loss: 2.5234 - acc: 0.18 - ETA: 22s - loss: 2.5192 - acc: 0.18 - ETA: 21s - loss: 2.5153 - acc: 0.18 - ETA: 21s - loss: 2.5132 - acc: 0.18 - ETA: 21s - loss: 2.5100 - acc: 0.18 - ETA: 21s - loss: 2.5075 - acc: 0.18 - ETA: 20s - loss: 2.5064 - acc: 0.18 - ETA: 20s - loss: 2.5030 - acc: 0.18 - ETA: 20s - loss: 2.5006 - acc: 0.18 - ETA: 20s - loss: 2.4988 - acc: 0.18 - ETA: 19s - loss: 2.4973 - acc: 0.18 - ETA: 19s - loss: 2.4936 - acc: 0.18 - ETA: 19s - loss: 2.4923 - acc: 0.18 - ETA: 19s - loss: 2.4899 - acc: 0.18 - ETA: 19s - loss: 2.4881 - acc: 0.18 - ETA: 19s - loss: 2.4858 - acc: 0.18 - ETA: 18s - loss: 2.4849 - acc: 0.18 - ETA: 18s - loss: 2.4828 - acc: 0.18 - ETA: 18s - loss: 2.4806 - acc: 0.18 - ETA: 18s - loss: 2.4784 - acc: 0.18 - ETA: 18s - loss: 2.4768 - acc: 0.19 - ETA: 18s - loss: 2.4745 - acc: 0.19 - ETA: 18s - loss: 2.4726 - acc: 0.19 - ETA: 17s - loss: 2.4712 - acc: 0.19 - ETA: 17s - loss: 2.4688 - acc: 0.19 - ETA: 17s - loss: 2.4672 - acc: 0.19 - ETA: 17s - loss: 2.4648 - acc: 0.19 - ETA: 17s - loss: 2.4638 - acc: 0.19 - ETA: 17s - loss: 2.4618 - acc: 0.19 - ETA: 17s - loss: 2.4603 - acc: 0.19 - ETA: 17s - loss: 2.4586 - acc: 0.19 - ETA: 16s - loss: 2.4567 - acc: 0.19 - ETA: 16s - loss: 2.4554 - acc: 0.19 - ETA: 16s - loss: 2.4533 - acc: 0.19 - ETA: 16s - loss: 2.4506 - acc: 0.19 - ETA: 16s - loss: 2.4491 - acc: 0.19 - ETA: 16s - loss: 2.4474 - acc: 0.19 - ETA: 16s - loss: 2.4456 - acc: 0.19 - ETA: 16s - loss: 2.4434 - acc: 0.19 - ETA: 16s - loss: 2.4414 - acc: 0.19 - ETA: 15s - loss: 2.4392 - acc: 0.19 - ETA: 15s - loss: 2.4373 - acc: 0.19 - ETA: 15s - loss: 2.4359 - acc: 0.19 - ETA: 15s - loss: 2.4342 - acc: 0.20 - ETA: 15s - loss: 2.4324 - acc: 0.20 - ETA: 15s - loss: 2.4309 - acc: 0.20 - ETA: 15s - loss: 2.4288 - acc: 0.20 - ETA: 15s - loss: 2.4268 - acc: 0.20 - ETA: 15s - loss: 2.4254 - acc: 0.20 - ETA: 15s - loss: 2.4241 - acc: 0.20 - ETA: 15s - loss: 2.4228 - acc: 0.20 - ETA: 14s - loss: 2.4213 - acc: 0.20 - ETA: 16s - loss: 2.4194 - acc: 0.20 - ETA: 16s - loss: 2.4177 - acc: 0.20 - ETA: 16s - loss: 2.4173 - acc: 0.20 - ETA: 16s - loss: 2.4157 - acc: 0.20 - ETA: 17s - loss: 2.4142 - acc: 0.20 - ETA: 17s - loss: 2.4130 - acc: 0.20 - ETA: 16s - loss: 2.4114 - acc: 0.20 - ETA: 16s - loss: 2.4105 - acc: 0.20 - ETA: 16s - loss: 2.4087 - acc: 0.20 - ETA: 16s - loss: 2.4073 - acc: 0.20 - ETA: 16s - loss: 2.4056 - acc: 0.20 - ETA: 16s - loss: 2.4038 - acc: 0.20 - ETA: 16s - loss: 2.4021 - acc: 0.20 - ETA: 16s - loss: 2.4006 - acc: 0.20 - ETA: 16s - loss: 2.3995 - acc: 0.20 - ETA: 15s - loss: 2.3980 - acc: 0.20 - ETA: 15s - loss: 2.3964 - acc: 0.20 - ETA: 15s - loss: 2.3953 - acc: 0.20 - ETA: 15s - loss: 2.3942 - acc: 0.20 - ETA: 15s - loss: 2.3931 - acc: 0.20 - ETA: 15s - loss: 2.3922 - acc: 0.21 - ETA: 15s - loss: 2.3910 - acc: 0.21 - ETA: 15s - loss: 2.3896 - acc: 0.21 - ETA: 15s - loss: 2.3886 - acc: 0.21 - ETA: 15s - loss: 2.3873 - acc: 0.21 - ETA: 14s - loss: 2.3862 - acc: 0.21 - ETA: 14s - loss: 2.3853 - acc: 0.21 - ETA: 14s - loss: 2.3839 - acc: 0.21 - ETA: 14s - loss: 2.3825 - acc: 0.21 - ETA: 14s - loss: 2.3810 - acc: 0.21 - ETA: 14s - loss: 2.3793 - acc: 0.21 - ETA: 14s - loss: 2.3783 - acc: 0.21 - ETA: 14s - loss: 2.3770 - acc: 0.21 - ETA: 14s - loss: 2.3761 - acc: 0.21 - ETA: 14s - loss: 2.3750 - acc: 0.21 - ETA: 14s - loss: 2.3736 - acc: 0.21 - ETA: 14s - loss: 2.3726 - acc: 0.21 - ETA: 13s - loss: 2.3717 - acc: 0.21 - ETA: 13s - loss: 2.3705 - acc: 0.21 - ETA: 13s - loss: 2.3693 - acc: 0.21 - ETA: 13s - loss: 2.3679 - acc: 0.21 - ETA: 13s - loss: 2.3670 - acc: 0.21 - ETA: 13s - loss: 2.3654 - acc: 0.21 - ETA: 13s - loss: 2.3645 - acc: 0.21 - ETA: 13s - loss: 2.3634 - acc: 0.21 - ETA: 13s - loss: 2.3619 - acc: 0.21 - ETA: 13s - loss: 2.3607 - acc: 0.21 - ETA: 13s - loss: 2.3596 - acc: 0.21 - ETA: 12s - loss: 2.3587 - acc: 0.21 - ETA: 12s - loss: 2.3575 - acc: 0.21 - ETA: 12s - loss: 2.3563 - acc: 0.21 - ETA: 12s - loss: 2.3547 - acc: 0.21 - ETA: 12s - loss: 2.3535 - acc: 0.21 - ETA: 12s - loss: 2.3520 - acc: 0.21 - ETA: 12s - loss: 2.3511 - acc: 0.21 - ETA: 12s - loss: 2.3502 - acc: 0.21 - ETA: 12s - loss: 2.3494 - acc: 0.21 - ETA: 12s - loss: 2.3483 - acc: 0.21 - ETA: 12s - loss: 2.3469 - acc: 0.21 - ETA: 12s - loss: 2.3458 - acc: 0.21 - ETA: 11s - loss: 2.3445 - acc: 0.22 - ETA: 11s - loss: 2.3433 - acc: 0.22 - ETA: 11s - loss: 2.3421 - acc: 0.22 - ETA: 11s - loss: 2.3414 - acc: 0.22 - ETA: 11s - loss: 2.3401 - acc: 0.22 - ETA: 11s - loss: 2.3388 - acc: 0.22 - ETA: 11s - loss: 2.3376 - acc: 0.22 - ETA: 11s - loss: 2.3368 - acc: 0.22 - ETA: 11s - loss: 2.3359 - acc: 0.22 - ETA: 11s - loss: 2.3348 - acc: 0.22 - ETA: 11s - loss: 2.3339 - acc: 0.22 - ETA: 11s - loss: 2.3330 - acc: 0.22 - ETA: 11s - loss: 2.3320 - acc: 0.22 - ETA: 10s - loss: 2.3309 - acc: 0.22 - ETA: 10s - loss: 2.3301 - acc: 0.22 - ETA: 10s - loss: 2.3290 - acc: 0.22 - ETA: 10s - loss: 2.3281 - acc: 0.22 - ETA: 10s - loss: 2.3271 - acc: 0.22 - ETA: 10s - loss: 2.3261 - acc: 0.22 - ETA: 10s - loss: 2.3251 - acc: 0.22 - ETA: 10s - loss: 2.3239 - acc: 0.22 - ETA: 10s - loss: 2.3231 - acc: 0.22 - ETA: 10s - loss: 2.3223 - acc: 0.22 - ETA: 10s - loss: 2.3214 - acc: 0.22 - ETA: 10s - loss: 2.3204 - acc: 0.22 - ETA: 10s - loss: 2.3195 - acc: 0.22 - ETA: 9s - loss: 2.3185 - acc: 0.2268 - ETA: 9s - loss: 2.3177 - acc: 0.227 - ETA: 9s - loss: 2.3170 - acc: 0.227 - ETA: 9s - loss: 2.3159 - acc: 0.227 - ETA: 9s - loss: 2.3151 - acc: 0.227 - ETA: 9s - loss: 2.3142 - acc: 0.227 - ETA: 9s - loss: 2.3131 - acc: 0.228 - ETA: 9s - loss: 2.3122 - acc: 0.228 - ETA: 9s - loss: 2.3114 - acc: 0.228 - ETA: 9s - loss: 2.3106 - acc: 0.228 - ETA: 9s - loss: 2.3096 - acc: 0.228 - ETA: 9s - loss: 2.3086 - acc: 0.229 - ETA: 9s - loss: 2.3075 - acc: 0.229 - ETA: 9s - loss: 2.3065 - acc: 0.229 - ETA: 8s - loss: 2.3054 - acc: 0.229 - ETA: 8s - loss: 2.3047 - acc: 0.229 - ETA: 8s - loss: 2.3036 - acc: 0.230 - ETA: 8s - loss: 2.3029 - acc: 0.230 - ETA: 8s - loss: 2.3020 - acc: 0.230 - ETA: 8s - loss: 2.3012 - acc: 0.230 - ETA: 8s - loss: 2.3004 - acc: 0.231 - ETA: 8s - loss: 2.2993 - acc: 0.231 - ETA: 8s - loss: 2.2986 - acc: 0.231 - ETA: 8s - loss: 2.2978 - acc: 0.231 - ETA: 8s - loss: 2.2969 - acc: 0.231 - ETA: 8s - loss: 2.2959 - acc: 0.232 - ETA: 8s - loss: 2.2949 - acc: 0.232 - ETA: 8s - loss: 2.2940 - acc: 0.232 - ETA: 8s - loss: 2.2931 - acc: 0.232 - ETA: 7s - loss: 2.2923 - acc: 0.233 - ETA: 7s - loss: 2.2916 - acc: 0.233 - ETA: 7s - loss: 2.2908 - acc: 0.233 - ETA: 7s - loss: 2.2900 - acc: 0.233 - ETA: 7s - loss: 2.2891 - acc: 0.233 - ETA: 7s - loss: 2.2882 - acc: 0.234 - ETA: 7s - loss: 2.2874 - acc: 0.234 - ETA: 7s - loss: 2.2866 - acc: 0.234 - ETA: 7s - loss: 2.2861 - acc: 0.234 - ETA: 7s - loss: 2.2854 - acc: 0.234 - ETA: 7s - loss: 2.2846 - acc: 0.234 - ETA: 7s - loss: 2.2837 - acc: 0.234 - ETA: 7s - loss: 2.2826 - acc: 0.2352\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358912/358912 [==============================] - ETA: 7s - loss: 2.2820 - acc: 0.235 - ETA: 7s - loss: 2.2813 - acc: 0.235 - ETA: 7s - loss: 2.2805 - acc: 0.235 - ETA: 6s - loss: 2.2798 - acc: 0.235 - ETA: 6s - loss: 2.2790 - acc: 0.236 - ETA: 6s - loss: 2.2783 - acc: 0.236 - ETA: 6s - loss: 2.2777 - acc: 0.236 - ETA: 6s - loss: 2.2771 - acc: 0.236 - ETA: 6s - loss: 2.2763 - acc: 0.236 - ETA: 6s - loss: 2.2754 - acc: 0.236 - ETA: 6s - loss: 2.2747 - acc: 0.237 - ETA: 6s - loss: 2.2739 - acc: 0.237 - ETA: 6s - loss: 2.2731 - acc: 0.237 - ETA: 6s - loss: 2.2724 - acc: 0.237 - ETA: 6s - loss: 2.2717 - acc: 0.237 - ETA: 6s - loss: 2.2711 - acc: 0.237 - ETA: 6s - loss: 2.2704 - acc: 0.238 - ETA: 6s - loss: 2.2697 - acc: 0.238 - ETA: 6s - loss: 2.2689 - acc: 0.238 - ETA: 6s - loss: 2.2681 - acc: 0.238 - ETA: 5s - loss: 2.2674 - acc: 0.238 - ETA: 5s - loss: 2.2666 - acc: 0.239 - ETA: 5s - loss: 2.2659 - acc: 0.239 - ETA: 5s - loss: 2.2650 - acc: 0.239 - ETA: 5s - loss: 2.2642 - acc: 0.239 - ETA: 5s - loss: 2.2633 - acc: 0.239 - ETA: 5s - loss: 2.2626 - acc: 0.240 - ETA: 5s - loss: 2.2619 - acc: 0.240 - ETA: 5s - loss: 2.2612 - acc: 0.240 - ETA: 5s - loss: 2.2607 - acc: 0.240 - ETA: 5s - loss: 2.2601 - acc: 0.240 - ETA: 5s - loss: 2.2594 - acc: 0.240 - ETA: 5s - loss: 2.2587 - acc: 0.240 - ETA: 5s - loss: 2.2580 - acc: 0.240 - ETA: 5s - loss: 2.2573 - acc: 0.241 - ETA: 5s - loss: 2.2565 - acc: 0.241 - ETA: 4s - loss: 2.2560 - acc: 0.241 - ETA: 4s - loss: 2.2554 - acc: 0.241 - ETA: 4s - loss: 2.2546 - acc: 0.241 - ETA: 4s - loss: 2.2538 - acc: 0.241 - ETA: 4s - loss: 2.2531 - acc: 0.241 - ETA: 4s - loss: 2.2524 - acc: 0.242 - ETA: 4s - loss: 2.2514 - acc: 0.242 - ETA: 4s - loss: 2.2505 - acc: 0.242 - ETA: 4s - loss: 2.2496 - acc: 0.242 - ETA: 4s - loss: 2.2489 - acc: 0.243 - ETA: 4s - loss: 2.2481 - acc: 0.243 - ETA: 4s - loss: 2.2473 - acc: 0.243 - ETA: 4s - loss: 2.2464 - acc: 0.243 - ETA: 4s - loss: 2.2456 - acc: 0.243 - ETA: 4s - loss: 2.2450 - acc: 0.243 - ETA: 3s - loss: 2.2444 - acc: 0.244 - ETA: 3s - loss: 2.2435 - acc: 0.244 - ETA: 3s - loss: 2.2429 - acc: 0.244 - ETA: 3s - loss: 2.2421 - acc: 0.244 - ETA: 3s - loss: 2.2413 - acc: 0.244 - ETA: 3s - loss: 2.2406 - acc: 0.244 - ETA: 3s - loss: 2.2398 - acc: 0.245 - ETA: 3s - loss: 2.2390 - acc: 0.245 - ETA: 3s - loss: 2.2383 - acc: 0.245 - ETA: 3s - loss: 2.2375 - acc: 0.245 - ETA: 3s - loss: 2.2367 - acc: 0.245 - ETA: 3s - loss: 2.2360 - acc: 0.246 - ETA: 3s - loss: 2.2352 - acc: 0.246 - ETA: 3s - loss: 2.2344 - acc: 0.246 - ETA: 3s - loss: 2.2336 - acc: 0.246 - ETA: 3s - loss: 2.2329 - acc: 0.246 - ETA: 2s - loss: 2.2323 - acc: 0.246 - ETA: 2s - loss: 2.2317 - acc: 0.246 - ETA: 2s - loss: 2.2311 - acc: 0.246 - ETA: 2s - loss: 2.2303 - acc: 0.247 - ETA: 2s - loss: 2.2297 - acc: 0.247 - ETA: 2s - loss: 2.2290 - acc: 0.247 - ETA: 2s - loss: 2.2284 - acc: 0.247 - ETA: 2s - loss: 2.2278 - acc: 0.247 - ETA: 2s - loss: 2.2271 - acc: 0.247 - ETA: 2s - loss: 2.2265 - acc: 0.248 - ETA: 2s - loss: 2.2258 - acc: 0.248 - ETA: 2s - loss: 2.2253 - acc: 0.248 - ETA: 2s - loss: 2.2246 - acc: 0.248 - ETA: 2s - loss: 2.2239 - acc: 0.248 - ETA: 2s - loss: 2.2231 - acc: 0.248 - ETA: 2s - loss: 2.2223 - acc: 0.249 - ETA: 1s - loss: 2.2217 - acc: 0.249 - ETA: 1s - loss: 2.2210 - acc: 0.249 - ETA: 1s - loss: 2.2204 - acc: 0.249 - ETA: 1s - loss: 2.2199 - acc: 0.249 - ETA: 1s - loss: 2.2190 - acc: 0.249 - ETA: 1s - loss: 2.2184 - acc: 0.249 - ETA: 1s - loss: 2.2179 - acc: 0.249 - ETA: 1s - loss: 2.2173 - acc: 0.250 - ETA: 1s - loss: 2.2167 - acc: 0.250 - ETA: 1s - loss: 2.2161 - acc: 0.250 - ETA: 1s - loss: 2.2155 - acc: 0.250 - ETA: 1s - loss: 2.2147 - acc: 0.250 - ETA: 1s - loss: 2.2141 - acc: 0.250 - ETA: 1s - loss: 2.2133 - acc: 0.250 - ETA: 1s - loss: 2.2126 - acc: 0.251 - ETA: 1s - loss: 2.2118 - acc: 0.251 - ETA: 1s - loss: 2.2110 - acc: 0.251 - ETA: 0s - loss: 2.2103 - acc: 0.251 - ETA: 0s - loss: 2.2095 - acc: 0.251 - ETA: 0s - loss: 2.2090 - acc: 0.252 - ETA: 0s - loss: 2.2084 - acc: 0.252 - ETA: 0s - loss: 2.2077 - acc: 0.252 - ETA: 0s - loss: 2.2070 - acc: 0.252 - ETA: 0s - loss: 2.2062 - acc: 0.252 - ETA: 0s - loss: 2.2054 - acc: 0.252 - ETA: 0s - loss: 2.2048 - acc: 0.253 - ETA: 0s - loss: 2.2041 - acc: 0.253 - ETA: 0s - loss: 2.2034 - acc: 0.253 - ETA: 0s - loss: 2.2028 - acc: 0.253 - ETA: 0s - loss: 2.2021 - acc: 0.253 - ETA: 0s - loss: 2.2016 - acc: 0.253 - ETA: 0s - loss: 2.2009 - acc: 0.253 - ETA: 0s - loss: 2.2003 - acc: 0.254 - ETA: 0s - loss: 2.1997 - acc: 0.254 - 21s 57us/step - loss: 2.1995 - acc: 0.2542 - val_loss: 2.1432 - val_acc: 0.2972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16846086828>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, ltarget,\n",
    "              batch_size=128,\n",
    "              epochs=1,\n",
    "              validation_data=(vdata, lvtarget),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>surface</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>soft_pvc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>concrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>concrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>soft_pvc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>tiled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>concrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>concrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>concrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>concrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>concrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>soft_pvc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>soft_pvc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>soft_pvc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>concrete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   surface\n",
       "0   4  soft_pvc\n",
       "1   1  concrete\n",
       "2   1  concrete\n",
       "3   4  soft_pvc\n",
       "4   3     tiled\n",
       "5   8      wood\n",
       "6   1  concrete\n",
       "7   1  concrete\n",
       "8   1  concrete\n",
       "9   1  concrete\n",
       "10  1  concrete\n",
       "11  4  soft_pvc\n",
       "12  4  soft_pvc\n",
       "13  4  soft_pvc\n",
       "14  1  concrete"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict_classes(test)\n",
    "preds = preds[0:-1:128]\n",
    "sub['surface'] = pd.DataFrame(preds)\n",
    "sub['surface'] = sub['surface'].map(decode_dic)\n",
    "sub.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape((2804, 128, 23))\n",
    "vdata = vdata.reshape((1006, 128, 23))\n",
    "test = test.reshape((3816, 128, 23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias: eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        if mask is not None: a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    inp = Input(shape=(128, 23))\n",
    "    x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(inp)\n",
    "    x = Attention(128)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    x = Dense(9, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds(X, y, X_test, k=5):\n",
    "    folds = list(StratifiedKFold(n_splits=k).split(X, y))\n",
    "    y_test = np.zeros((X_test.shape[0], 9))\n",
    "    y_oof = np.zeros((X.shape[0]))\n",
    "    \n",
    "    for i, (train_idx, val_idx) in  enumerate(folds):\n",
    "        print(f'Fold {i+1}')\n",
    "        model = make_model()\n",
    "        model.fit(X[train_idx], y[train_idx], batch_size=128, epochs=100, \n",
    "                  validation_data=[vdata, y[val_idx]], verbose=0)\n",
    "        \n",
    "        pred_val = np.argmax(model.predict(X[val_idx]), axis=1)\n",
    "        score = accuracy_score(pred_val, y[val_idx])\n",
    "        y_oof[val_idx] = pred_val\n",
    "        \n",
    "        print(f'Scored {score:.3f} on validation data')\n",
    "        \n",
    "        y_test += model.predict(X_test)\n",
    "        \n",
    "    return y_oof, y_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_3 to have 3 dimensions, but got array with shape (530, 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-d3a8eee03863>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_oof\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_folds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-70-0bc26f3fc627>\u001b[0m in \u001b[0;36mk_folds\u001b[1;34m(X, y, X_test, k)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         model.fit(X[train_idx], y[train_idx], batch_size=128, epochs=100, \n\u001b[1;32m---> 10\u001b[1;33m                   validation_data=[validation_part_df, y[val_idx]], verbose=0)\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mpred_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m                 \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 972\u001b[1;33m                 batch_size=batch_size)\n\u001b[0m\u001b[0;32m    973\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m                 \u001b[0mval_ins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_3 to have 3 dimensions, but got array with shape (530, 13)"
     ]
    }
   ],
   "source": [
    "y_oof, y_test = k_folds(data, target, test, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Local CV is {accuracy_score(y_oof, y_train): .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['surface'] = y_test\n",
    "sub['surface'] = sub['surface'].map(decode_dic)\n",
    "sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
