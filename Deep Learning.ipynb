{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages are ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Lets first import our packages for everything. If you don't have a package installed you can use !pip install package_name\n",
    "\n",
    "# These are for automatic hyperparameter optimization\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "\n",
    "# Our standard packages for data science.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# These are for augmenting and finding the data. We won't be using SKLearn much for actually modelling.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# These are the actual packages for deep learning. We will mostly use the high level keras package for tensorflow.\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.optimizers import *\n",
    "from keras.layers import LeakyReLU, Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "print('Packages are ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "# Lets import our data\n",
    "target = pd.read_csv('target.csv')\n",
    "data = pd.read_csv('data.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "vdata = pd.read_csv('vdata.csv')\n",
    "vtarget = pd.read_csv('vtarget.csv')\n",
    "ltarget = pd.read_csv('ltarget.csv', header=None)\n",
    "lvtarget = pd.read_csv('lvtarget.csv', header=None)\n",
    "wdata = pd.read_csv('whole_data.csv')\n",
    "wtarget = pd.read_csv('whole_target.csv', header=None)\n",
    "\n",
    "winedata = pd.read_csv('winedata.csv')\n",
    "\n",
    "ccdata = pd.read_csv('creditcard.csv')\n",
    "\n",
    "irisdata = pd.read_csv('irisdata.csv')\n",
    "iristarget = pd.read_csv('iristarget.csv')\n",
    "\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets go ahead and set up our data. First lets make our target and drop it from wine.\n",
    "winetarget = winedata['quality']\n",
    "winedata = winedata.drop('quality', axis=1)\n",
    "\n",
    "# Lets trim off time and amount from ccdata as those are independent features we don't want the model to learn.\n",
    "cctarget = ccdata['Class']\n",
    "ccdata = ccdata.drop(['Time','Amount', 'Class'], axis=1)\n",
    "\n",
    "iristarget = iristarget['target']\n",
    "\n",
    "target = target['surface']\n",
    "vtarget = vtarget['surface']\n",
    "ltarget = ltarget[0]\n",
    "lvtarget = lvtarget[0]\n",
    "wdata = wdata.drop(['series_id', 'group_id', 'surface'], axis=1)\n",
    "wtarget = wtarget[0]\n",
    "\n",
    "irisd, id_test, irist, it_test = train_test_split(irisdata, iristarget, test_size=0.33, random_state=42)\n",
    "wined, wd_test, winet, wt_test = train_test_split(winedata, winetarget, test_size=0.33, random_state=42)\n",
    "ccd, ccd_test, cct, cct_test = train_test_split(ccdata, cctarget, test_size=0.33, random_state=42)\n",
    "\n",
    "data = data.values\n",
    "vdata = vdata.values\n",
    "wdata = wdata.values\n",
    "test = test.values\n",
    "# irisdata = irisdata.values\n",
    "# ccdata = ccdata.values\n",
    "# winedata = winedata.values\n",
    "# id_test = id_test.values\n",
    "# ccd_test = ccd_test.values\n",
    "# wd_test = wd_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "(6497, 11)\n",
      "(6497,)\n",
      "(284807, 28)\n",
      "(284807,)\n",
      "(2804,)\n",
      "(1006,)\n",
      "(358912,)\n",
      "(128768,)\n",
      "(487680,)\n",
      "(487680, 23)\n",
      "(128768, 23)\n",
      "(358912, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(488448, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets make sure our train and target variables are even for every dataset\n",
    "print(irisdata.shape)\n",
    "print(iristarget.shape)\n",
    "\n",
    "print(winedata.shape)\n",
    "print(winetarget.shape)\n",
    "\n",
    "print(ccdata.shape)\n",
    "print(cctarget.shape)\n",
    "\n",
    "print(target.shape)\n",
    "print(vtarget.shape)\n",
    "print(ltarget.shape)\n",
    "print(lvtarget.shape)\n",
    "print(wtarget.shape)\n",
    "print(wdata.shape)\n",
    "print(vdata.shape)\n",
    "print(data.shape)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! All our data is set up properally and is ready to be worked on!\n",
    "\n",
    "One large problem we always need to think about when creating a keras model is if we want to go 'deep' or 'wide'.\n",
    "\n",
    "Here is a good example of the difference.\n",
    "\n",
    "<img src='http://www.coldvision.io/wp-content/uploads/2016/07/dnn_ann_vs_dnn.png' />\n",
    "\n",
    "Having only a single, very large hidden layer is what we call a 'wide' model. Having many small hidden layers are what we call a 'deep' model. A wide model can learn any function but it will also be prone to overfitting. A deep model generalizes better but can take longer to train depending on the layers you have. So in deep learning its all about finding the balance between the two models. You want to make the model wide enough to where it learns but no wider and then as deep as you can computaionally afford!\n",
    "\n",
    "One of the other ways to improve neural networks and reduce the chance of overfitting is by introducing a concept called 'dropout'. It randomly by some value between 0 and 1 it will stop neurons in the network from working. This forces the network to learn new paths down its layers. This can stop the model from getting stuck at local maxima during Backpropagation.\n",
    "\n",
    "https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5\n",
    "\n",
    "https://arxiv.org/pdf/1902.06720.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our decoder for the target of the IMUSD\n",
    "decode_dic = {0: 'fine_concrete',\n",
    "              1: 'concrete',\n",
    "              2: 'soft_tiles',\n",
    "              3: 'tiled',\n",
    "              4: 'soft_pvc',\n",
    "              5: 'hard_tiles_large_space',\n",
    "              6: 'carpet',\n",
    "              7: 'hard_tiles',\n",
    "              8: 'wood'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make an intelligent way to search our model creationg for the smaller datasets. Below is a great way to automatically find the best keras model.\n",
    "\n",
    "Here is another resource for keras hyperparameter optimization https://medium.com/@mikkokotila/a-comprehensive-list-of-hyperparameter-optimization-tuning-solutions-88e067f19d9\n",
    "\n",
    "Lets start with wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we build a template for the gridsearch to work with\n",
    "\n",
    "input_shape = 11\n",
    "num_classes = 1\n",
    "\n",
    "def build_model(optimizer, learning_rate, activation, dropout_rate, num_unit):\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_unit, activation=activation, input_shape=(input_shape,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(num_unit, activation=activation))\n",
    "    model.add(Dropout(dropout_rate)) \n",
    "    \n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of options we are giving our model.\n",
    "# The more parameters you have the longer it will take by x!\n",
    "# I also highly recommend that you use this with gpu enabled.\n",
    "\n",
    "batch_size = [100, 200]\n",
    "\n",
    "epochs = [10]\n",
    "\n",
    "learning_rate = [0.1, 0.001, 0.01]\n",
    "\n",
    "dropout_rate = [0.3, 0.2, 0.1]\n",
    "\n",
    "num_unit = [64, 32]\n",
    "\n",
    "activation = ['relu', 'tanh']\n",
    "\n",
    "optimizer = [SGD, RMSprop, Adam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make our gridsearch\n",
    "# parameters is a dict with all values\n",
    "\n",
    "parameters = dict(batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  dropout_rate = dropout_rate,\n",
    "                  num_unit = num_unit,\n",
    "                  learning_rate = learning_rate,\n",
    "                  activation = activation,\n",
    "                  optimizer = optimizer)\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, verbose=0)\n",
    "\n",
    "models = GridSearchCV(estimator = model, param_grid=parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be given our best wine model from the search.\n",
    "If this code fails then you will need to change the keras file.\n",
    "\n",
    "https://stackoverflow.com/a/52132383/9975219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model :\n",
      "{   'activation': 'tanh',\n",
      "    'batch_size': 200,\n",
      "    'dropout_rate': 0.1,\n",
      "    'epochs': 10,\n",
      "    'learning_rate': 0.01,\n",
      "    'num_unit': 64,\n",
      "    'optimizer': <class 'keras.optimizers.Adam'>}\n"
     ]
    }
   ],
   "source": [
    "best_model = models.fit(wined, winet, validation_data=(wd_test, wt_test), shuffle=True)\n",
    "print('Best model :')\n",
    "pp.pprint(best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with this info lets see how well our best model does then we will redo this process for CCD and IRIS\n",
    "\n",
    "input_shape = 11\n",
    "num_classes = 1\n",
    "opt = keras.optimizers.Adam(lr=0.01)\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(input_shape,)))\n",
    "model.add(Dropout(0.1))\n",
    "    \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1)) \n",
    "    \n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    \n",
    "model.compile(loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4352 samples, validate on 2145 samples\n",
      "Epoch 1/10\n",
      "4352/4352 [==============================] - 0s 106us/step - loss: 3.3596 - acc: 0.5161 - val_loss: 0.7389 - val_acc: 0.5697\n",
      "Epoch 2/10\n",
      "4352/4352 [==============================] - 0s 23us/step - loss: 0.8027 - acc: 0.5795 - val_loss: 0.6418 - val_acc: 0.6476\n",
      "Epoch 3/10\n",
      "4352/4352 [==============================] - 0s 21us/step - loss: 0.6795 - acc: 0.6121 - val_loss: 0.6538 - val_acc: 0.6480\n",
      "Epoch 4/10\n",
      "4352/4352 [==============================] - 0s 21us/step - loss: 0.6653 - acc: 0.6335 - val_loss: 0.6500 - val_acc: 0.6205\n",
      "Epoch 5/10\n",
      "4352/4352 [==============================] - 0s 20us/step - loss: 0.6624 - acc: 0.6234 - val_loss: 0.6334 - val_acc: 0.6490\n",
      "Epoch 6/10\n",
      "4352/4352 [==============================] - 0s 20us/step - loss: 0.6522 - acc: 0.6353 - val_loss: 0.6297 - val_acc: 0.6517\n",
      "Epoch 7/10\n",
      "4352/4352 [==============================] - 0s 20us/step - loss: 0.6486 - acc: 0.6298 - val_loss: 0.6320 - val_acc: 0.6499\n",
      "Epoch 8/10\n",
      "4352/4352 [==============================] - 0s 20us/step - loss: 0.6421 - acc: 0.6328 - val_loss: 0.6200 - val_acc: 0.6503\n",
      "Epoch 9/10\n",
      "4352/4352 [==============================] - 0s 20us/step - loss: 0.6273 - acc: 0.6427 - val_loss: 0.6109 - val_acc: 0.6531\n",
      "Epoch 10/10\n",
      "4352/4352 [==============================] - 0s 22us/step - loss: 0.6250 - acc: 0.6404 - val_loss: 0.5961 - val_acc: 0.6527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a2daa3c8d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(wined, winet,\n",
    "              batch_size=200,\n",
    "              epochs=10,\n",
    "              validation_data=(wd_test, wt_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2145/2145 [==============================] - 0s 32us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5961327495135905, 0.6526806526806527]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(wd_test, wt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model is capping around .75%. We could improve this by hand tunning the model. Or we could do some more feature engineering to the model to give it more to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do CCD first\n",
    "\n",
    "input_shape = 28\n",
    "num_classes = 1\n",
    "\n",
    "def build_model(optimizer, learning_rate, activation, dropout_rate, num_unit):\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_unit, activation=activation, input_shape=(input_shape,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(num_unit, activation=activation))\n",
    "    model.add(Dropout(dropout_rate)) \n",
    "    \n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could change our hyperparameters buuuuuutttttt nah.\n",
    "\n",
    "batch_size = [100, 200]\n",
    "\n",
    "epochs = [10]\n",
    "\n",
    "learning_rate = [0.1, 0.001, 0.01]\n",
    "\n",
    "dropout_rate = [0.3, 0.2, 0.1]\n",
    "\n",
    "num_unit = [64, 32]\n",
    "\n",
    "activation = ['relu', 'tanh']\n",
    "\n",
    "optimizer = [SGD, RMSprop, Adam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another GridSearch!\n",
    "\n",
    "parameters = dict(batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  dropout_rate = dropout_rate,\n",
    "                  num_unit = num_unit,\n",
    "                  learning_rate = learning_rate,\n",
    "                  activation = activation,\n",
    "                  optimizer = optimizer)\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, verbose=0)\n",
    "\n",
    "models = GridSearchCV(estimator = model, param_grid=parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model :\n",
      "{   'activation': 'relu',\n",
      "    'batch_size': 100,\n",
      "    'dropout_rate': 0.1,\n",
      "    'epochs': 10,\n",
      "    'learning_rate': 0.001,\n",
      "    'num_unit': 64,\n",
      "    'optimizer': <class 'keras.optimizers.Adam'>}\n"
     ]
    }
   ],
   "source": [
    "best_model = models.fit(ccd, cct, validation_data=(ccd_test, cct_test), shuffle=True)\n",
    "print('Best model :')\n",
    "pp.pprint(best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Awesome. That one probably took longer huh?\n",
    "\n",
    "input_shape = 28\n",
    "num_classes = 1\n",
    "opt = keras.optimizers.Adam(lr=0.01)\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(input_shape,)))\n",
    "model.add(Dropout(0.1))\n",
    "    \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1)) \n",
    "    \n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    \n",
    "model.compile(loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 190820 samples, validate on 93987 samples\n",
      "Epoch 1/10\n",
      "190820/190820 [==============================] - 4s 22us/step - loss: 0.0077 - acc: 0.9980 - val_loss: 0.0039 - val_acc: 0.9993\n",
      "Epoch 2/10\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0045 - acc: 0.9993 - val_loss: 0.0039 - val_acc: 0.9994\n",
      "Epoch 3/10\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.0037 - val_acc: 0.9993\n",
      "Epoch 4/10\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.0039 - val_acc: 0.9994\n",
      "Epoch 5/10\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.0037 - val_acc: 0.9993\n",
      "Epoch 6/10\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0039 - acc: 0.9993 - val_loss: 0.0040 - val_acc: 0.9994\n",
      "Epoch 7/10\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 0.0032 - val_acc: 0.9994\n",
      "Epoch 8/10\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 0.0034 - val_acc: 0.9994\n",
      "Epoch 9/10\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 0.0034 - val_acc: 0.9994\n",
      "Epoch 10/10\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.0035 - val_acc: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a2a34e0c88>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ccd, cct,\n",
    "              batch_size=100,\n",
    "              epochs=10,\n",
    "              validation_data=(ccd_test, cct_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93987/93987 [==============================] - 3s 29us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.003477105172812994, 0.9994041729175311]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ccd_test, cct_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our data is doing better then just guessing everything 'not fruad'. We could improve our model like we did for the SKLearn models with SMOTE or some other under sampling approach. We could also make our data more sensitive to the minority \n",
    "class using the Neual Net weights. Which is what we are doing in this notebook!\n",
    "\n",
    "https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets SMOTE our data. We are only increasing the minority sample to 50% of the majority class.\n",
    "# This will hold that we have a minority class and that we need to use the weights to adjust for it.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(sampling_strategy=0.5, random_state=2)\n",
    "\n",
    "ccd_SMOTE, cct_SMOTE = sm.fit_resample(ccd,cct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "285715"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see how many new minority samples there are.\n",
    "print(len(cct))\n",
    "len(cct_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets rerun that code but add the 'class_weight' dict.\n",
    "\n",
    "input_shape = 28\n",
    "num_classes = 1\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(input_shape,)))\n",
    "model.add(Dropout(0.1))\n",
    "    \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1)) \n",
    "    \n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    \n",
    "model.compile(loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 190820 samples, validate on 93987 samples\n",
      "Epoch 1/30\n",
      "190820/190820 [==============================] - 4s 22us/step - loss: 0.0269 - acc: 0.9954 - val_loss: 0.0029 - val_acc: 0.9994\n",
      "Epoch 2/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0058 - acc: 0.9993 - val_loss: 0.0029 - val_acc: 0.9994\n",
      "Epoch 3/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0053 - acc: 0.9993 - val_loss: 0.0034 - val_acc: 0.9994\n",
      "Epoch 4/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0049 - acc: 0.9993 - val_loss: 0.0027 - val_acc: 0.9994\n",
      "Epoch 5/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0046 - acc: 0.9994 - val_loss: 0.0027 - val_acc: 0.9996\n",
      "Epoch 6/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0042 - acc: 0.9994 - val_loss: 0.0025 - val_acc: 0.9995\n",
      "Epoch 7/30\n",
      "190820/190820 [==============================] - 4s 21us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.0024 - val_acc: 0.9994\n",
      "Epoch 8/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.0029 - val_acc: 0.9994\n",
      "Epoch 9/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0037 - acc: 0.9995 - val_loss: 0.0024 - val_acc: 0.9995\n",
      "Epoch 10/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0036 - acc: 0.9994 - val_loss: 0.0025 - val_acc: 0.9995\n",
      "Epoch 11/30\n",
      "190820/190820 [==============================] - 4s 19us/step - loss: 0.0034 - acc: 0.9995 - val_loss: 0.0026 - val_acc: 0.9994\n",
      "Epoch 12/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0034 - acc: 0.9995 - val_loss: 0.0024 - val_acc: 0.9995\n",
      "Epoch 13/30\n",
      "190820/190820 [==============================] - 3s 18us/step - loss: 0.0032 - acc: 0.9995 - val_loss: 0.0025 - val_acc: 0.9994\n",
      "Epoch 14/30\n",
      "190820/190820 [==============================] - 3s 18us/step - loss: 0.0031 - acc: 0.9995 - val_loss: 0.0029 - val_acc: 0.9994\n",
      "Epoch 15/30\n",
      "190820/190820 [==============================] - 3s 18us/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.0028 - val_acc: 0.9994\n",
      "Epoch 16/30\n",
      "190820/190820 [==============================] - 3s 18us/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.0027 - val_acc: 0.9994\n",
      "Epoch 17/30\n",
      "190820/190820 [==============================] - 3s 18us/step - loss: 0.0027 - acc: 0.9996 - val_loss: 0.0026 - val_acc: 0.9995\n",
      "Epoch 18/30\n",
      "190820/190820 [==============================] - 3s 18us/step - loss: 0.0026 - acc: 0.9996 - val_loss: 0.0027 - val_acc: 0.9994\n",
      "Epoch 19/30\n",
      "190820/190820 [==============================] - 3s 18us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0028 - val_acc: 0.9994\n",
      "Epoch 20/30\n",
      "190820/190820 [==============================] - 4s 19us/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.0028 - val_acc: 0.9995\n",
      "Epoch 21/30\n",
      "190820/190820 [==============================] - 4s 19us/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.0026 - val_acc: 0.9995\n",
      "Epoch 22/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0024 - acc: 0.9996 - val_loss: 0.0029 - val_acc: 0.9995\n",
      "Epoch 23/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0028 - val_acc: 0.9995\n",
      "Epoch 24/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0030 - val_acc: 0.9994\n",
      "Epoch 25/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0028 - val_acc: 0.9995\n",
      "Epoch 26/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0031 - val_acc: 0.9995\n",
      "Epoch 27/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.0031 - val_acc: 0.9995\n",
      "Epoch 28/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0030 - val_acc: 0.9995\n",
      "Epoch 29/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0033 - val_acc: 0.9994\n",
      "Epoch 30/30\n",
      "190820/190820 [==============================] - 4s 20us/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.0032 - val_acc: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a2abacceb8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0: 1.0,\n",
    "                1: 2.0}\n",
    "\n",
    "model.fit(ccd, cct,\n",
    "              batch_size=200,\n",
    "              epochs=30,\n",
    "              validation_data=(ccd_test, cct_test),\n",
    "              class_weight=class_weight,\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93987/93987 [==============================] - 3s 30us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.003164525856868862, 0.9994360922255205]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ccd_test, cct_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some improvement!! We could get to 100% if you fine tune and do the process of gridsearch again with a deeper and more diverse pool for the search. That would take a day and if I was doing this as a product to find fraud then I would!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we need to do somethings different. For multiclass classification we need to use 'sparse_categorical_crossentropy' or just regular 'categorical_crossentropy' for the loss instead of 'binary_crossentropy'. This is because our model will fail in general but also it won't learn. We will also need to change the final output activation to 'softmax' instead of 'sigmoid' as sigmoid does not work for multiclass classification! This is very important and easy to forget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another template, this time for iris.\n",
    "\n",
    "input_shape = 4\n",
    "num_classes = 3\n",
    "\n",
    "def build_model(optimizer, learning_rate, activation, dropout_rate, num_unit):\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_unit, activation=activation, input_shape=(input_shape,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(num_unit, activation=activation))\n",
    "    model.add(Dropout(dropout_rate)) \n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=optimizer(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our hyperparameters\n",
    "\n",
    "batch_size = [100, 200]\n",
    "\n",
    "epochs = [10]\n",
    "\n",
    "learning_rate = [0.1, 0.001, 0.01]\n",
    "\n",
    "dropout_rate = [0.3, 0.2, 0.1]\n",
    "\n",
    "num_unit = [64, 32]\n",
    "\n",
    "activation = ['relu', 'tanh']\n",
    "\n",
    "optimizer = [SGD, RMSprop, Adam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make our gridsearch again.\n",
    "\n",
    "parameters = dict(batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  dropout_rate = dropout_rate,\n",
    "                  num_unit = num_unit,\n",
    "                  learning_rate = learning_rate,\n",
    "                  activation = activation,\n",
    "                  optimizer = optimizer)\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, verbose=0)\n",
    "\n",
    "models = GridSearchCV(estimator = model, param_grid=parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model :\n",
      "{   'activation': 'tanh',\n",
      "    'batch_size': 100,\n",
      "    'dropout_rate': 0.3,\n",
      "    'epochs': 10,\n",
      "    'learning_rate': 0.01,\n",
      "    'num_unit': 32,\n",
      "    'optimizer': <class 'keras.optimizers.Adam'>}\n"
     ]
    }
   ],
   "source": [
    "# Thank god this one won't take long!\n",
    "best_model = models.fit(irisd, irist, validation_data=(id_test, it_test), shuffle=True)\n",
    "print('Best model :')\n",
    "pp.pprint(best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last time lets make our best model!\n",
    "\n",
    "input_shape = 4\n",
    "num_classes = 3\n",
    "opt = keras.optimizers.Adam(lr=0.01)\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='tanh', input_shape=(input_shape,)))\n",
    "model.add(Dropout(0.3))\n",
    "    \n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dropout(0.3)) \n",
    "    \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.8199 - acc: 0.6300 - val_loss: 0.6218 - val_acc: 0.7600\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.7776 - acc: 0.6700 - val_loss: 0.5497 - val_acc: 0.9200\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.6923 - acc: 0.7500 - val_loss: 0.4979 - val_acc: 0.9600\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.6645 - acc: 0.7700 - val_loss: 0.4707 - val_acc: 0.9600\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.6162 - acc: 0.7600 - val_loss: 0.4532 - val_acc: 0.9600\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.6073 - acc: 0.7400 - val_loss: 0.4321 - val_acc: 0.9600\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.6059 - acc: 0.7700 - val_loss: 0.4017 - val_acc: 0.9600\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.5713 - acc: 0.8100 - val_loss: 0.3705 - val_acc: 0.9200\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.4976 - acc: 0.8000 - val_loss: 0.3417 - val_acc: 0.9400\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.4382 - acc: 0.8500 - val_loss: 0.3194 - val_acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a2acc93fd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(irisd, irist,\n",
    "              batch_size=100,\n",
    "              epochs=10,\n",
    "              validation_data=(id_test, it_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 80us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3193650126457214, 0.9599999904632568]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(id_test, it_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool Now lets tackle the big boy. We will start with just a standard dense model with 3 hidden layers. You can use the earlier method to create the best model again but it will take a VERY long time!!!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to make sure the data is the right shape for training.\n",
    "data = data.reshape((358912, 23))\n",
    "vdata = vdata.reshape((128768, 23))\n",
    "test = test.reshape((488448, 23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "num_classes = 9\n",
    "input_dim = 23\n",
    "\n",
    "# Here is a standard keras fully connected model.\n",
    "# The reason I am using 'tanh' instead of 'relu' is that about half the variables are negative.\n",
    "model.add(Dense(64, input_dim=input_dim, use_bias=False, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(64, use_bias=False, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(64, use_bias=False, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the model using RMSprop.\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 358912 samples, validate on 128768 samples\n",
      "Epoch 1/10\n",
      "358912/358912 [==============================] - 20s 57us/step - loss: 2.0995 - acc: 0.2844 - val_loss: 2.1300 - val_acc: 0.3275\n",
      "Epoch 2/10\n",
      "358912/358912 [==============================] - 18s 51us/step - loss: 1.8062 - acc: 0.3672 - val_loss: 2.1333 - val_acc: 0.3069\n",
      "Epoch 3/10\n",
      "358912/358912 [==============================] - 18s 51us/step - loss: 1.6541 - acc: 0.4176 - val_loss: 2.2527 - val_acc: 0.2974\n",
      "Epoch 4/10\n",
      "358912/358912 [==============================] - 19s 52us/step - loss: 1.5452 - acc: 0.4532 - val_loss: 2.3845 - val_acc: 0.3049\n",
      "Epoch 5/10\n",
      "358912/358912 [==============================] - 19s 52us/step - loss: 1.4611 - acc: 0.4797 - val_loss: 2.3974 - val_acc: 0.3018\n",
      "Epoch 6/10\n",
      "358912/358912 [==============================] - 19s 52us/step - loss: 1.3918 - acc: 0.5027 - val_loss: 2.4957 - val_acc: 0.3289\n",
      "Epoch 7/10\n",
      "358912/358912 [==============================] - 19s 52us/step - loss: 1.3362 - acc: 0.5217 - val_loss: 2.4949 - val_acc: 0.3357\n",
      "Epoch 8/10\n",
      "358912/358912 [==============================] - 20s 55us/step - loss: 1.2853 - acc: 0.5404 - val_loss: 2.5854 - val_acc: 0.3378\n",
      "Epoch 9/10\n",
      "358912/358912 [==============================] - 19s 54us/step - loss: 1.2425 - acc: 0.5551 - val_loss: 2.5598 - val_acc: 0.3360\n",
      "Epoch 10/10\n",
      "358912/358912 [==============================] - 19s 53us/step - loss: 1.2052 - acc: 0.5671 - val_loss: 2.5344 - val_acc: 0.3436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a2accaf828>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AAAAND start the training.\n",
    "model.fit(data, ltarget,\n",
    "              batch_size=128,\n",
    "              epochs=10,\n",
    "              validation_data=(vdata, lvtarget),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make an lstm to read the data as a time series. I used an attention layer because it's cool but really it's not needed. Here is a great article on attention for DL.\n",
    "\n",
    "https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data again for the Time Series model as LSTM's need 3 input dimensions.\n",
    "data = data.reshape((2804, 128, 23))\n",
    "vdata = vdata.reshape((1006, 128, 23))\n",
    "test = test.reshape((3816, 128, 23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias: eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        if mask is not None: a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make the keras model. This time we will not use a sequential model but instead a functional api model.\n",
    "def make_model():\n",
    "    inp = Input(shape=(128, 23))\n",
    "    x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(inp)\n",
    "    x = Attention(128)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    x = Dense(9, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our k-folds and our validation data.\n",
    "def k_folds(X, y, X_test, k=5):\n",
    "    folds = list(StratifiedKFold(n_splits=k).split(X, y))\n",
    "    y_test = np.zeros((X_test.shape[0], 9))\n",
    "    y_oof = np.zeros((X.shape[0]))\n",
    "    \n",
    "    for i, (train_idx, val_idx) in  enumerate(folds):\n",
    "        print(f'Fold {i+1}')\n",
    "        model = make_model()\n",
    "        model.fit(X[train_idx], y[train_idx], batch_size=128, epochs=100, \n",
    "                  validation_data=[X[val_idx], y[val_idx]], verbose=0)\n",
    "        \n",
    "        pred_val = np.argmax(model.predict(X[val_idx]), axis=1)\n",
    "        score = accuracy_score(pred_val, y[val_idx])\n",
    "        y_oof[val_idx] = pred_val\n",
    "        \n",
    "        print(f'Scored {score:.3f} on validation data')\n",
    "        \n",
    "        y_test += model.predict(X_test)\n",
    "        \n",
    "    return y_oof, y_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Scored 0.621 on validation data\n",
      "Fold 2\n",
      "Scored 0.611 on validation data\n",
      "Fold 3\n",
      "Scored 0.629 on validation data\n",
      "Fold 4\n",
      "Scored 0.608 on validation data\n",
      "Fold 5\n",
      "Scored 0.618 on validation data\n"
     ]
    }
   ],
   "source": [
    "# LET THE TEST BEGIN!\n",
    "y_oof, y_test = k_folds(data, target, test, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 3, ..., 6, 1, 8], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cool, so here is our output.\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make the same model but use our validation data instead of K-folds! You will notice that the validation score is much lower but actually this is from the model NOT overfitting. So the K-folds for deep learning may not be the best for every use case. Instead you want to create actual validation data that is never trained on by the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are hijacking the model from the last model.\n",
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using rmsprop again but feel free to try new things!\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2804 samples, validate on 1006 samples\n",
      "Epoch 1/10\n",
      "2804/2804 [==============================] - 2s 630us/step - loss: 2.1829 - acc: 0.1327 - val_loss: 2.1593 - val_acc: 0.1740\n",
      "Epoch 2/10\n",
      "2804/2804 [==============================] - 1s 179us/step - loss: 2.1535 - acc: 0.1551 - val_loss: 2.1381 - val_acc: 0.2217\n",
      "Epoch 3/10\n",
      "2804/2804 [==============================] - 0s 177us/step - loss: 2.1201 - acc: 0.1926 - val_loss: 2.1224 - val_acc: 0.2157\n",
      "Epoch 4/10\n",
      "2804/2804 [==============================] - 0s 177us/step - loss: 2.0964 - acc: 0.2086 - val_loss: 2.1104 - val_acc: 0.2575\n",
      "Epoch 5/10\n",
      "2804/2804 [==============================] - 0s 175us/step - loss: 2.0802 - acc: 0.2215 - val_loss: 2.1016 - val_acc: 0.2604\n",
      "Epoch 6/10\n",
      "2804/2804 [==============================] - 0s 177us/step - loss: 2.0687 - acc: 0.2279 - val_loss: 2.0919 - val_acc: 0.2634\n",
      "Epoch 7/10\n",
      "2804/2804 [==============================] - 0s 176us/step - loss: 2.0467 - acc: 0.2500 - val_loss: 2.0831 - val_acc: 0.2674\n",
      "Epoch 8/10\n",
      "2804/2804 [==============================] - 0s 176us/step - loss: 2.0351 - acc: 0.2539 - val_loss: 2.0761 - val_acc: 0.2684\n",
      "Epoch 9/10\n",
      "2804/2804 [==============================] - 0s 175us/step - loss: 2.0322 - acc: 0.2507 - val_loss: 2.0688 - val_acc: 0.2704\n",
      "Epoch 10/10\n",
      "2804/2804 [==============================] - 0s 171us/step - loss: 2.0216 - acc: 0.2425 - val_loss: 2.0627 - val_acc: 0.2734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a3089e8080>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, target,\n",
    "              batch_size=128,\n",
    "              epochs=10,\n",
    "              validation_data=(vdata, vtarget),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it! Feel Free to check out my other notebooks or the keras documentation for more Deep learning goodness!\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "Also not to make you feel like deep learning is limited here is a list of different networks that can be bootstrapped together to solve any problem!\n",
    "\n",
    "https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
