{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import eli5\n",
    "import shap\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split, GroupKFold, GroupShuffleSplit\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "import gc\n",
    "from catboost import CatBoostClassifier\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import HTML\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import convolve\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "# Lets import our data\n",
    "target = pd.read_csv('target.csv')\n",
    "data = pd.read_csv('data.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "vdata = pd.read_csv('vdata.csv')\n",
    "vtarget = pd.read_csv('vtarget.csv')\n",
    "ltarget = pd.read_csv('ltarget.csv', header=None)\n",
    "lvtarget = pd.read_csv('lvtarget.csv', header=None)\n",
    "wdata = pd.read_csv('whole_data.csv')\n",
    "wtarget = pd.read_csv('whole_target.csv', header=None)\n",
    "\n",
    "winedata = pd.read_csv('winedata.csv')\n",
    "\n",
    "ccdata = pd.read_csv('creditcard.csv')\n",
    "\n",
    "irisdata = pd.read_csv('irisdata.csv')\n",
    "iristarget = pd.read_csv('iristarget.csv')\n",
    "\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets go ahead and set up our data. First lets make our target and drop it from wine.\n",
    "winetarget = winedata['quality']\n",
    "winedata = winedata.drop('quality', axis=1)\n",
    "\n",
    "# Lets trim off time and amount from ccdata as those are independent features we don't want the model to learn.\n",
    "cctarget = ccdata['Class']\n",
    "ccdata = ccdata.drop(['Time','Amount', 'Class'], axis=1)\n",
    "\n",
    "iristarget = iristarget['target']\n",
    "\n",
    "target = target['surface']\n",
    "vtarget = vtarget['surface']\n",
    "ltarget = ltarget[0]\n",
    "lvtarget = lvtarget[0]\n",
    "wdata = wdata.drop(['series_id', 'group_id', 'surface'], axis=1)\n",
    "wtarget = wtarget[0]\n",
    "\n",
    "data = data.values\n",
    "vdata = vdata.values\n",
    "wdata = wdata.values\n",
    "test = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "(6497, 11)\n",
      "(6497,)\n",
      "(284807, 28)\n",
      "(284807,)\n",
      "(2804,)\n",
      "(1006,)\n",
      "(358912,)\n",
      "(128768,)\n",
      "(487680,)\n",
      "(487680, 23)\n",
      "(128768, 23)\n",
      "(358912, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(488448, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets make sure our train and target variables are even for every dataset\n",
    "print(irisdata.shape)\n",
    "print(iristarget.shape)\n",
    "\n",
    "print(winedata.shape)\n",
    "print(winetarget.shape)\n",
    "\n",
    "print(ccdata.shape)\n",
    "print(cctarget.shape)\n",
    "\n",
    "print(target.shape)\n",
    "print(vtarget.shape)\n",
    "print(ltarget.shape)\n",
    "print(lvtarget.shape)\n",
    "print(wtarget.shape)\n",
    "print(wdata.shape)\n",
    "print(vdata.shape)\n",
    "print(data.shape)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'acc', accuracy_score(labels, preds.argmax(1)), True\n",
    "\n",
    "def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "\n",
    "    oof = np.zeros((len(X), 9))\n",
    "    prediction = np.zeros((len(X_test), 9))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = data, vdata\n",
    "        y_train, y_valid = ltarget, lvtarget\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMClassifier(**params, n_estimators = 10000, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='multi_logloss',\n",
    "                    verbose=5000, early_stopping_rounds=200)\n",
    "            \n",
    "            y_pred_valid = model.predict_proba(X_valid)\n",
    "            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict_proba(X_valid)\n",
    "            score = accuracy_score(y_valid, y_pred_valid.argmax(1))\n",
    "            print(f'Fold {fold_n}. Accuracy: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostClassifier(iterations=20000,  eval_metric='MAE', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid\n",
    "        scores.append(accuracy_score(y_valid, y_pred_valid.argmax(1)))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 123,\n",
    "          'min_data_in_leaf': 12,\n",
    "          'objective': 'multiclass',\n",
    "          'max_depth': 22,\n",
    "          'learning_rate': 0.04680350949723872,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8933018355190274,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.9498109326932401,\n",
    "          'reg_lambda': 0.8058490960546196,\n",
    "          \"num_class\": 9,\n",
    "          'nthread': -1,\n",
    "          'min_split_gain': 0.009913227240564853,\n",
    "          'subsample': 0.9027358830703129\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sun May  5 21:12:53 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's multi_logloss: 0.873976\ttraining's multi_logloss: 0.873976\tvalid_1's multi_logloss: 1.96774\tvalid_1's multi_logloss: 1.96774\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (128768,9) could not be broadcast to indexing result of shape (71786,9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b180aad181f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moof_lgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_lgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mltarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lgb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_feature_importance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-e7197433c7f0>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, model)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0moof\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred_valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shape mismatch: value array of shape (128768,9) could not be broadcast to indexing result of shape (71786,9)"
     ]
    }
   ],
   "source": [
    "oof_lgb, prediction_lgb, feature_importance = train_model(X=data, X_test=test, y=ltarget, params=params, model_type='lgb', plot_feature_importance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(**params, n_estimators = 20000, n_jobs = -1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df, y['surface'], test_size=0.1, stratify=y['surface'])\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=5000, early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(model, targets=[0, 1], feature_names=list(X_train.columns), top=40, feature_filter=lambda x: x != '<BIAS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(probability=True)\n",
    "oof_svc, prediction_svc = train_model(X=train_df, X_test=test_df, y=y['surface'], params=None, model_type='sklearn', model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
