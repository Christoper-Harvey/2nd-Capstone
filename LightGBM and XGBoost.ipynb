{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split, GroupKFold, GroupShuffleSplit\n",
    "from sklearn.metrics import *\n",
    "import gc\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "# Lets import our data\n",
    "target = pd.read_csv('target.csv')\n",
    "data = pd.read_csv('data.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "vdata = pd.read_csv('vdata.csv')\n",
    "vtarget = pd.read_csv('vtarget.csv')\n",
    "ltarget = pd.read_csv('ltarget.csv', header=None)\n",
    "lvtarget = pd.read_csv('lvtarget.csv', header=None)\n",
    "wdata = pd.read_csv('whole_data.csv')\n",
    "wtarget = pd.read_csv('whole_target.csv', header=None)\n",
    "\n",
    "winedata = pd.read_csv('winedata.csv')\n",
    "\n",
    "ccdata = pd.read_csv('creditcard.csv')\n",
    "\n",
    "irisdata = pd.read_csv('irisdata.csv')\n",
    "iristarget = pd.read_csv('iristarget.csv')\n",
    "\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because XGBoost uses a .split() command we will get rid of any whitespace in our column names.\n",
    "winedata.columns = ['fixed_acidity','volatile_acidity','citric_acid','residual_sugar','chlorides','free_sulfur_dioxide','total_sulfur_dioxide','density','pH','sulphates','alcohol', 'quality']\n",
    "irisdata.columns = ['sepal_length_(cm)','sepal_width_(cm)','petal_length_(cm)','petal_width_(cm)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets go ahead and set up our data. First lets make our target and drop it from wine.\n",
    "winetarget = winedata['quality']\n",
    "winedata = winedata.drop('quality', axis=1)\n",
    "\n",
    "# Lets trim off time and amount from ccdata as those are independent features we don't want the model to learn.\n",
    "cctarget = ccdata['Class']\n",
    "ccdata = ccdata.drop(['Time','Amount', 'Class'], axis=1)\n",
    "\n",
    "iristarget = iristarget['target']\n",
    "\n",
    "target = target['surface']\n",
    "vtarget = vtarget['surface']\n",
    "ltarget = ltarget[0]\n",
    "lvtarget = lvtarget[0]\n",
    "wdata = wdata.drop(['series_id', 'group_id', 'surface'], axis=1)\n",
    "wtarget = wtarget[0]\n",
    "\n",
    "irisd, id_test, irist, it_test = train_test_split(irisdata, iristarget, test_size=0.33, random_state=42)\n",
    "wined, wd_test, winet, wt_test = train_test_split(winedata, winetarget, test_size=0.33, random_state=42)\n",
    "ccd, ccd_test, cct, cct_test = train_test_split(ccdata, cctarget, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "(6497, 11)\n",
      "(6497,)\n",
      "(284807, 28)\n",
      "(284807,)\n",
      "(2804,)\n",
      "(1006,)\n",
      "(358912,)\n",
      "(128768,)\n",
      "(487680,)\n",
      "(487680, 23)\n",
      "(128768, 23)\n",
      "(358912, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(488448, 23)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets make sure our train and target variables are even for every dataset\n",
    "print(irisdata.shape)\n",
    "print(iristarget.shape)\n",
    "\n",
    "print(winedata.shape)\n",
    "print(winetarget.shape)\n",
    "\n",
    "print(ccdata.shape)\n",
    "print(cctarget.shape)\n",
    "\n",
    "print(target.shape)\n",
    "print(vtarget.shape)\n",
    "print(ltarget.shape)\n",
    "print(lvtarget.shape)\n",
    "print(wtarget.shape)\n",
    "print(wdata.shape)\n",
    "print(vdata.shape)\n",
    "print(data.shape)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like in our other notebooks we are going to SMOTE the ccdata\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(sampling_strategy=0.5, random_state=2)\n",
    "\n",
    "ccd_SMOTE, cct_SMOTE = sm.fit_resample(ccd,cct)\n",
    "\n",
    "ccd_SMOTE = pd.DataFrame(ccd_SMOTE, columns=ccd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "285715"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see how many new minority samples there are.\n",
    "print(len(cct))\n",
    "len(cct_SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try both LightGBM and XGBoost. If you are new to machine learning and did not read my 'Standard SKLearn' notebook I would encourage you to do so. It covers more of the basics and has some great learning resources!\n",
    "\n",
    "XGBoost is a boosting machine, The love child of kagglers everywhere! It is an incredibly powerful algorithm for learning just about anything. It is easy to set up and run but tuning is a nightmare! It has a strong learning curve but it is very worth the effort to learn. I linked an article below about how to tune XGBoost, Give it a read!\n",
    "\n",
    "LightGBM is gradient boosting machine. You might ask what is the difference? Good question! One is much, much faster on big data. They both are awful to tune though. Again I linked the tuning guides below!\n",
    "\n",
    "Here is a great article about the differences and the history of the two models.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/\n",
    "\n",
    "Below are the docs for both as well as some introductionary tutorials.\n",
    "\n",
    "https://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/\n",
    "\n",
    "https://www.kaggle.com/pintu161/implementation-of-lightgbm-for-begineers\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets time our models for fun.\n",
    "from datetime import datetime \n",
    "# Put this above the code.\n",
    "start = datetime.now()\n",
    "\n",
    "# Put this below.\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "execution_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with LightGBM. This is a great tool for large databases.\n",
    "\n",
    "Lets start things off with the same order we used in SKLearn. Iris, Wine, CCD, IMUSD.\n",
    "\n",
    "We are going to do both regular Cross-Validation and fixed set validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "Multiclass objective and metrics don't match",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-deae44bff467>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# The training command and testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mlgbm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_round\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[0;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[1;32m-> 1552\u001b[1;33m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1553\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m                 ctypes.byref(self.handle)))\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    999\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1001\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m   1002\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_list_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init_from_np2d\u001b[1;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[0;32m    853\u001b[0m             \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[0mref_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m             ctypes.byref(self.handle)))\n\u001b[0m\u001b[0;32m    856\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\doomb\\anaconda3\\envs\\capstone2\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \"\"\"\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Multiclass objective and metrics don't match"
     ]
    }
   ],
   "source": [
    "# This will be a fixed set validation model. The next model will use CV.\n",
    "\n",
    "# For keeping time\n",
    "start = datetime.now()\n",
    "# Loading our data as a custom LGM dataset.\n",
    "train_data=lgb.Dataset(irisd,label=irist)\n",
    "\n",
    "# Our model's parameters.\n",
    "param = {'objective':\"multiclass\", 'metric':[\"multi_logloss\",'auc'], 'num_class':3}\n",
    "# param['metric'] = ['auc', 'multi_logloss']\n",
    "\n",
    "# The amount of trees we will explore.\n",
    "num_round=50\n",
    "\n",
    "# The training command and testing\n",
    "lgbm=lgb.train(param,train_data,num_round)\n",
    "y_pred=lgbm.predict(id_test)\n",
    "\n",
    "# This tells us how well our model is doing.\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(it_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)\n",
    "# datetime.timedelta( , , ) representation => (days , seconds , microseconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.17%\n",
      "0:00:08.679766\n"
     ]
    }
   ],
   "source": [
    "# This is our first CV model, I'll keep the formatting of both for future models.\n",
    "\n",
    "start = datetime.now()\n",
    "# The number of CV folds we want to do.\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=11)\n",
    "\n",
    "# These are our model's hyperparameters which we will tune by hand to give us the best model we get get!\n",
    "params = {'num_leaves': 123,\n",
    "          'min_data_in_leaf': 12,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': 22,\n",
    "          'learning_rate': 0.04680350949723872,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8933018355190274,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.9498109326932401,\n",
    "          'reg_lambda': 0.8058490960546196,\n",
    "          \"num_class\": 9,\n",
    "          'nthread': -1,\n",
    "          'min_split_gain': 0.009913227240564853,\n",
    "          'subsample': 0.9027358830703129\n",
    "         }\n",
    "\n",
    "# Our data, this time not as a custom dataset.\n",
    "X = data\n",
    "Y = ltarget\n",
    "\n",
    "# Our model\n",
    "model = lgb.LGBMClassifier(**params, n_estimators = 20, n_jobs = -1)\n",
    "\n",
    "# This trains our model then tells us how well it is doing.\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%%\" % (results.mean()*100))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = datetime.now()\n",
    "\n",
    "train_data=lgb.Dataset(ccd,label=cct)\n",
    "\n",
    "param = {'num_leaves':150, 'objective':'binary','max_depth':7,'learning_rate':.05,'max_bin':200}\n",
    "param['metric'] = ['auc', 'binary_logloss']\n",
    "\n",
    "num_round=50\n",
    "\n",
    "lgbm=lgb.train(param,train_data,num_round)\n",
    "y_pred=lgbm.predict(ccd_test)\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(cct_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=11)\n",
    "\n",
    "params = {'num_leaves': 123,\n",
    "          'min_data_in_leaf': 12,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': 22,\n",
    "          'learning_rate': 0.04680350949723872,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8933018355190274,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.9498109326932401,\n",
    "          'reg_lambda': 0.8058490960546196,\n",
    "          \"num_class\": 9,\n",
    "          'nthread': -1,\n",
    "          'min_split_gain': 0.009913227240564853,\n",
    "          'subsample': 0.9027358830703129\n",
    "         }\n",
    "\n",
    "X = data\n",
    "Y = ltarget\n",
    "\n",
    "model = lgb.LGBMClassifier(**params, n_estimators = 20, n_jobs = -1)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%%\" % (results.mean()*100))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "train_data=lgb.Dataset(ccd,label=cct)\n",
    "\n",
    "param = {'num_leaves':150, 'objective':'binary','max_depth':7,'learning_rate':.05,'max_bin':200}\n",
    "param['metric'] = ['auc', 'binary_logloss']\n",
    "\n",
    "num_round=50\n",
    "\n",
    "lgbm=lgb.train(param,train_data,num_round)\n",
    "y_pred=lgbm.predict(ccd_test)\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(cct_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=11)\n",
    "\n",
    "params = {'num_leaves': 123,\n",
    "          'min_data_in_leaf': 12,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': 22,\n",
    "          'learning_rate': 0.04680350949723872,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8933018355190274,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.9498109326932401,\n",
    "          'reg_lambda': 0.8058490960546196,\n",
    "          \"num_class\": 9,\n",
    "          'nthread': -1,\n",
    "          'min_split_gain': 0.009913227240564853,\n",
    "          'subsample': 0.9027358830703129\n",
    "         }\n",
    "\n",
    "X = data\n",
    "Y = ltarget\n",
    "\n",
    "model = lgb.LGBMClassifier(**params, n_estimators = 20, n_jobs = -1)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%%\" % (results.mean()*100))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "train_data=lgb.Dataset(ccd,label=cct)\n",
    "\n",
    "param = {'num_leaves':150, 'objective':'binary','max_depth':7,'learning_rate':.05,'max_bin':200}\n",
    "param['metric'] = ['auc', 'binary_logloss']\n",
    "\n",
    "num_round=50\n",
    "\n",
    "lgbm=lgb.train(param,train_data,num_round)\n",
    "y_pred=lgbm.predict(ccd_test)\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(cct_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=11)\n",
    "\n",
    "params = {'num_leaves': 123,\n",
    "          'min_data_in_leaf': 12,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': 22,\n",
    "          'learning_rate': 0.04680350949723872,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8933018355190274,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.9498109326932401,\n",
    "          'reg_lambda': 0.8058490960546196,\n",
    "          \"num_class\": 9,\n",
    "          'nthread': -1,\n",
    "          'min_split_gain': 0.009913227240564853,\n",
    "          'subsample': 0.9027358830703129\n",
    "         }\n",
    "\n",
    "X = data\n",
    "Y = ltarget\n",
    "\n",
    "model = lgb.LGBMClassifier(**params, n_estimators = 20, n_jobs = -1)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%%\" % (results.mean()*100))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Time!\n",
    "\n",
    "We will do the same order and tests as above with similar hyperparameters to see how each algorithm compares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.04\tvalid_data-merror:0\n",
      "Multiple eval metrics have been passed: 'valid_data-merror' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-merror hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-merror:0.04\tvalid_data-merror:0\n",
      "\n",
      "Accuracy: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1bf8ebfc8d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# These are the list of parameters for the model. Reference the param guide at the start of the notebook for changing them.\n",
    "param = {'max_depth':100, 'eta':.001, 'silent':1, 'objective':'multi:softmax', 'num_class':3, 'nthread':-1}\n",
    "\n",
    "# These create custom matrices for the model to work with.\n",
    "train_data = xgb.DMatrix(data=irisd, label=irist)\n",
    "valid_data = xgb.DMatrix(data=id_test, label=it_test)\n",
    "\n",
    "# You add the matrices above into a list. The model uses to train and evaluate itself.\n",
    "watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "# This is the main model we will use for XGBoost, The set up below this code block is for a CV model.\n",
    "model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=param)\n",
    "# This is what we use to test the model's performance.\n",
    "y_pred = model.predict(valid_data, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(it_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# We can even plot how important each feature is to the model!\n",
    "xgb.plot_importance(model)\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.33%\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# We will not use DMatrix for the CV model. Instead load the data as such\n",
    "X = irisdata\n",
    "Y = iristarget\n",
    "\n",
    "# CV model, The params are given below instead of in a seperate dict.\n",
    "model = xgb.XGBClassifier(max_depth=100, n_estimators=0, learning_rate=0.01, objective='multi:softmax', num_class=3, n_jobs=-1)\n",
    "# You can change the KFolds but the standards are 3, 5, or 10. In this case 3 works best.\n",
    "kfold = StratifiedKFold(n_splits=3, random_state=7)\n",
    "# This is our accuracy metric.\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%%\" % (results.mean()*100))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now then if you are comming from the SKLearn notebook you know of my struggle with the wine dataset! Lets see if we can conquer it once and for all!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "param = {'max_depth':100, 'eta':.001, 'silent':1, 'objective':'binary:logistic', 'num_class':1, 'nthread':-1}\n",
    "\n",
    "train_data = xgb.DMatrix(data=wined, label=winet)\n",
    "valid_data = xgb.DMatrix(data=wd_test, label=wt_test)\n",
    "\n",
    "watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=param)\n",
    "y_pred = model.predict(valid_data, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(wt_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "xgb.plot_importance(model)\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES! Lets see how the model does with CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# load data\n",
    "X = winedata\n",
    "Y = winetarget\n",
    "\n",
    "# CV model\n",
    "model = xgb.XGBClassifier(max_depth=100, n_estimators=0, learning_rate=0.01, objective='binary:logistic', n_jobs=-1)\n",
    "kfold = KFold(n_splits=3, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%%\" % (results.mean()*100))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that good huh? If you can't tell I'm not a huge fan of CV\n",
    "\n",
    "Ah well, Onto CCD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "param = {'max_depth':1, 'eta':.001, 'silent':1, 'objective':'binary:logistic', 'num_class':1, 'nthread':-1}\n",
    "\n",
    "train_data = xgb.DMatrix(data=ccd, label=cct)\n",
    "valid_data = xgb.DMatrix(data=ccd_test, label=cct_test)\n",
    "\n",
    "watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "model = xgb.train(dtrain=train_data, num_boost_round=20, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=param)\n",
    "y_pred = model.predict(valid_data, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(cct_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "xgb.plot_importance(model)\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# load data\n",
    "X = ccdata\n",
    "Y = cctarget\n",
    "\n",
    "# CV model\n",
    "model = xgb.XGBClassifier(max_depth=1, n_estimators=0, learning_rate=0.001, objective='binary:logistic', n_jobs=-1)\n",
    "kfold = KFold(n_splits=3, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%%\" % (results.mean()*100))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# Lets also do the SMOTE data to see the difference.\n",
    "param = {'max_depth':1, 'eta':.001, 'silent':1, 'objective':'binary:logistic', 'num_class':1, 'nthread':-1}\n",
    "\n",
    "train_data = xgb.DMatrix(data=ccd_SMOTE, label=cct_SMOTE)\n",
    "valid_data = xgb.DMatrix(data=ccd_test, label=cct_test)\n",
    "\n",
    "watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "model = xgb.train(dtrain=train_data, num_boost_round=20, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=param)\n",
    "y_pred = model.predict(valid_data, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(cct_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "xgb.plot_importance(model)\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, So there are some big differences between SMOTE, non-SMOTE and CV methods!\n",
    "\n",
    "Lets see how our IMUSD does with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "param = {'max_depth':1, 'eta':.001, 'silent':1, 'objective':'multi:softmax', 'num_class':9, 'nthread':-1}\n",
    "\n",
    "train_data = xgb.DMatrix(data=data, label=ltarget)\n",
    "valid_data = xgb.DMatrix(data=vdata, label=lvtarget)\n",
    "\n",
    "watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "model = xgb.train(dtrain=train_data, num_boost_round=20, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=param)\n",
    "y_pred = model.predict(valid_data, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(lvtarget, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "xgb.plot_importance(model)\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# load data\n",
    "X = wdata\n",
    "Y = wtarget\n",
    "\n",
    "# CV model\n",
    "model = xgb.XGBClassifier(max_depth=1, n_estimators=0, learning_rate=0.001, objective='multi:softmax', num_class=9, n_jobs=-1)\n",
    "kfold = StratifiedKFold(n_splits=3, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%%\" % (results.mean()*100))\n",
    "\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow we see comparable results with our deep model. Not near the actual best but still good. We can see the large difference between RandomForest and regular trees and Boosted trees\n",
    "\n",
    "Thats it for this notebook! Thanks for reading, If you haven't already check out the other notebooks :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
