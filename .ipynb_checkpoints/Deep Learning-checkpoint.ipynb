{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages are ready!\n"
     ]
    }
   ],
   "source": [
    "# Lets first import our packages for everything. If you don't have a package installed you can use !pip install package_name\n",
    "\n",
    "# These are for automatic hyperparameter optimization\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "\n",
    "# Our standard packages for data science.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# These are for augmenting and finding the data. We won't be using SKLearn much for actually modelling.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# These are the actual packages for deep learning. We will mostly use the high level keras package for tensorflow.\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.optimizers import *\n",
    "from keras.layers import LeakyReLU, Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "print('Packages are ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "# Lets import our data\n",
    "target = pd.read_csv('target.csv')\n",
    "data = pd.read_csv('data.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "vdata = pd.read_csv('vdata.csv')\n",
    "vtarget = pd.read_csv('vtarget.csv')\n",
    "ltarget = pd.read_csv('ltarget.csv', header=None)\n",
    "lvtarget = pd.read_csv('lvtarget.csv', header=None)\n",
    "wdata = pd.read_csv('whole_data.csv')\n",
    "wtarget = pd.read_csv('whole_target.csv', header=None)\n",
    "\n",
    "winedata = pd.read_csv('winedata.csv')\n",
    "\n",
    "ccdata = pd.read_csv('creditcard.csv')\n",
    "\n",
    "irisdata = pd.read_csv('irisdata.csv')\n",
    "iristarget = pd.read_csv('iristarget.csv')\n",
    "\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets go ahead and set up our data. First lets make our target and drop it from wine.\n",
    "winetarget = winedata['quality']\n",
    "winedata = winedata.drop('quality', axis=1)\n",
    "\n",
    "# Lets trim off time and amount from ccdata as those are independent features we don't want the model to learn.\n",
    "cctarget = ccdata['Class']\n",
    "ccdata = ccdata.drop(['Time','Amount', 'Class'], axis=1)\n",
    "\n",
    "iristarget = iristarget['target']\n",
    "\n",
    "target = target['surface']\n",
    "vtarget = vtarget['surface']\n",
    "ltarget = ltarget[0]\n",
    "lvtarget = lvtarget[0]\n",
    "wdata = wdata.drop(['series_id', 'group_id', 'surface'], axis=1)\n",
    "wtarget = wtarget[0]\n",
    "\n",
    "data = data.values\n",
    "vdata = vdata.values\n",
    "wdata = wdata.values\n",
    "test = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "(6497, 11)\n",
      "(6497,)\n",
      "(284807, 28)\n",
      "(284807,)\n",
      "(2804,)\n",
      "(1006,)\n",
      "(358912,)\n",
      "(128768,)\n",
      "(487680,)\n",
      "(487680, 23)\n",
      "(128768, 23)\n",
      "(358912, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(488448, 23)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets make sure our train and target variables are even for every dataset\n",
    "print(irisdata.shape)\n",
    "print(iristarget.shape)\n",
    "\n",
    "print(winedata.shape)\n",
    "print(winetarget.shape)\n",
    "\n",
    "print(ccdata.shape)\n",
    "print(cctarget.shape)\n",
    "\n",
    "print(target.shape)\n",
    "print(vtarget.shape)\n",
    "print(ltarget.shape)\n",
    "print(lvtarget.shape)\n",
    "print(wtarget.shape)\n",
    "print(wdata.shape)\n",
    "print(vdata.shape)\n",
    "print(data.shape)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! All our data is set up properally and is ready to be worked on!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our decoder for the target of the IMUSD\n",
    "decode_dic = {0: 'fine_concrete',\n",
    "              1: 'concrete',\n",
    "              2: 'soft_tiles',\n",
    "              3: 'tiled',\n",
    "              4: 'soft_pvc',\n",
    "              5: 'hard_tiles_large_space',\n",
    "              6: 'carpet',\n",
    "              7: 'hard_tiles',\n",
    "              8: 'wood'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make an intelligent way to search our model creationg for the smaller datasets. Below is a great way to automatically find the best keras model.\n",
    "\n",
    "Here is another resource for keras hyperparameter optimization https://medium.com/@mikkokotila/a-comprehensive-list-of-hyperparameter-optimization-tuning-solutions-88e067f19d9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we build a template for the gridsearch to work with\n",
    "\n",
    "def build_model(optimizer, learning_rate, activation, dropout_rate,\n",
    "                initilizer,num_unit):\n",
    "    keras.backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_unit, kernel_initializer=initilizer,\n",
    "                    activation=activation, input_shape=(784,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_unit, kernel_initializer=initilizer,\n",
    "                    activation=activation))\n",
    "    model.add(Dropout(dropout_rate)) \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of options we are giving our model, [:1] is for testing each value inside.\n",
    "\n",
    "batch_size = [20, 50, 100][:1]\n",
    "\n",
    "epochs = [1, 20, 50][:1]\n",
    "\n",
    "initilizer = ['lecun_uniform', 'normal', 'he_normal', 'he_uniform'][:1]\n",
    "\n",
    "learning_rate = [0.1, 0.001, 0.02][:1]\n",
    "\n",
    "dropout_rate = [0.3, 0.2, 0.8][:1]\n",
    "\n",
    "num_unit = [32, 10, 5][:1]\n",
    "\n",
    "activation = ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'][:1]\n",
    "\n",
    "optimizer = [SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make our gridsearch\n",
    "# parameters is a dict with all values\n",
    "\n",
    "parameters = dict(batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  dropout_rate = dropout_rate,\n",
    "                  num_unit = num_unit,\n",
    "                  initilizer = initilizer,\n",
    "                  learning_rate = learning_rate,\n",
    "                  activation = activation,\n",
    "                  optimizer = optimizer)\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, verbose=0)\n",
    "\n",
    "models = GridSearchCV(estimator = model, param_grid=parameters, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model :\n",
      "{   'activation': 'relu',\n",
      "    'batch_size': 20,\n",
      "    'dropout_rate': 0.3,\n",
      "    'epochs': 1,\n",
      "    'initilizer': 'lecun_uniform',\n",
      "    'learning_rate': 0.1,\n",
      "    'num_unit': 32,\n",
      "    'optimizer': <class 'keras.optimizers.SGD'>}\n"
     ]
    }
   ],
   "source": [
    "# Here we will be given our best model from the search.\n",
    "# The larger the model and the more options given the longer it will take to train.\n",
    "best_model = models.fit(x_train, y_train)\n",
    "print('Best model :')\n",
    "pp.pprint(best_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool Now lets tackle the big boy. We will start with just a standard dense model with 3 hidden layers. You can use the earlier method to create the best model again but it will take a VERY long time!!!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to make sure the data is the right shape for training.\n",
    "data = data.reshape((358912, 23))\n",
    "vdata = vdata.reshape((128768, 23))\n",
    "test = test.reshape((488448, 23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "num_classes = 9\n",
    "\n",
    "# Here is a standard keras fully connected model.\n",
    "model.add(Dense(64, input_dim=23, use_bias=False, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(64, use_bias=False, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(64, use_bias=False, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the model using RMSprop.\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 358912 samples, validate on 128768 samples\n",
      "Epoch 1/5\n",
      "358912/358912 [==============================] - 19s 54us/step - loss: 2.0838 - acc: 0.2919 - val_loss: 2.1242 - val_acc: 0.3125\n",
      "Epoch 2/5\n",
      "358912/358912 [==============================] - 18s 51us/step - loss: 1.7872 - acc: 0.3740 - val_loss: 2.1605 - val_acc: 0.3109\n",
      "Epoch 3/5\n",
      "358912/358912 [==============================] - 18s 50us/step - loss: 1.6305 - acc: 0.4249 - val_loss: 2.2642 - val_acc: 0.2885\n",
      "Epoch 4/5\n",
      "358912/358912 [==============================] - 18s 50us/step - loss: 1.5195 - acc: 0.4623 - val_loss: 2.3984 - val_acc: 0.3063\n",
      "Epoch 5/5\n",
      "358912/358912 [==============================] - 18s 50us/step - loss: 1.4345 - acc: 0.4888 - val_loss: 2.4863 - val_acc: 0.3033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17905300320>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AAAAND start the training.\n",
    "model.fit(data, ltarget,\n",
    "              batch_size=128,\n",
    "              epochs=5,\n",
    "              validation_data=(vdata, lvtarget),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make an lstm to read the data as a time series. I used an attention layer because it's cool but really it's not needed. Here is a great article on attention for DL.\n",
    "\n",
    "https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data again for the Time Series model as LSTM's need 3 input dimensions.\n",
    "data = data.reshape((2804, 128, 23))\n",
    "vdata = vdata.reshape((1006, 128, 23))\n",
    "test = test.reshape((3816, 128, 23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias: eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        if mask is not None: a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make the keras model. This time we will not use a sequential model but instead a functional api model.\n",
    "def make_model():\n",
    "    inp = Input(shape=(128, 23))\n",
    "    x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(inp)\n",
    "    x = Attention(128)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    x = Dense(9, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our k-folds and our validation data.\n",
    "def k_folds(X, y, X_test, k=5):\n",
    "    folds = list(StratifiedKFold(n_splits=k).split(X, y))\n",
    "    y_test = np.zeros((X_test.shape[0], 9))\n",
    "    y_oof = np.zeros((X.shape[0]))\n",
    "    \n",
    "    for i, (train_idx, val_idx) in  enumerate(folds):\n",
    "        print(f'Fold {i+1}')\n",
    "        model = make_model()\n",
    "        model.fit(X[train_idx], y[train_idx], batch_size=128, epochs=100, \n",
    "                  validation_data=[X[val_idx], y[val_idx]], verbose=0)\n",
    "        \n",
    "        pred_val = np.argmax(model.predict(X[val_idx]), axis=1)\n",
    "        score = accuracy_score(pred_val, y[val_idx])\n",
    "        y_oof[val_idx] = pred_val\n",
    "        \n",
    "        print(f'Scored {score:.3f} on validation data')\n",
    "        \n",
    "        y_test += model.predict(X_test)\n",
    "        \n",
    "    return y_oof, y_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Scored 0.616 on validation data\n",
      "Fold 2\n",
      "Scored 0.607 on validation data\n",
      "Fold 3\n",
      "Scored 0.624 on validation data\n",
      "Fold 4\n",
      "Scored 0.611 on validation data\n",
      "Fold 5\n",
      "Scored 0.567 on validation data\n"
     ]
    }
   ],
   "source": [
    "# LET THE TEST BEGIN!\n",
    "y_oof, y_test = k_folds(data, target, test, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 3, ..., 6, 1, 8], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cool, so here is our output.\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make the same model but use our validation data instead of K-folds! You will notice that the validation score is much lower but actually this is from the model NOT overfitting. So the K-folds for deep learning may not be the best for every use case. Instead you want to create actual validation data that is never trained on by the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are hijacking the model from the last model.\n",
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using rmsprop again but feel free to try new things!\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2804 samples, validate on 1006 samples\n",
      "Epoch 1/10\n",
      "2804/2804 [==============================] - 2s 607us/step - loss: 2.2928 - acc: 0.1815 - val_loss: 2.2042 - val_acc: 0.1899\n",
      "Epoch 2/10\n",
      "2804/2804 [==============================] - 0s 167us/step - loss: 2.2434 - acc: 0.1944 - val_loss: 2.1843 - val_acc: 0.1899\n",
      "Epoch 3/10\n",
      "2804/2804 [==============================] - 0s 167us/step - loss: 2.1827 - acc: 0.1976 - val_loss: 2.1665 - val_acc: 0.1899\n",
      "Epoch 4/10\n",
      "2804/2804 [==============================] - 0s 168us/step - loss: 2.1458 - acc: 0.2111 - val_loss: 2.1527 - val_acc: 0.1899\n",
      "Epoch 5/10\n",
      "2804/2804 [==============================] - 0s 166us/step - loss: 2.1319 - acc: 0.2136 - val_loss: 2.1377 - val_acc: 0.1899\n",
      "Epoch 6/10\n",
      "2804/2804 [==============================] - 0s 170us/step - loss: 2.1051 - acc: 0.2265 - val_loss: 2.1261 - val_acc: 0.1879\n",
      "Epoch 7/10\n",
      "2804/2804 [==============================] - 0s 166us/step - loss: 2.0639 - acc: 0.2468 - val_loss: 2.1134 - val_acc: 0.1859\n",
      "Epoch 8/10\n",
      "2804/2804 [==============================] - 0s 167us/step - loss: 2.0619 - acc: 0.2521 - val_loss: 2.1035 - val_acc: 0.1859\n",
      "Epoch 9/10\n",
      "2804/2804 [==============================] - 0s 167us/step - loss: 2.0369 - acc: 0.2539 - val_loss: 2.0954 - val_acc: 0.2674\n",
      "Epoch 10/10\n",
      "2804/2804 [==============================] - 0s 166us/step - loss: 2.0127 - acc: 0.2743 - val_loss: 2.0855 - val_acc: 0.2803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17903a6a9b0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, target,\n",
    "              batch_size=128,\n",
    "              epochs=10,\n",
    "              validation_data=(vdata, vtarget),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it! Feel Free to check out my other notebooks or the keras documentation for more Deep learning goodness!\n",
    "\n",
    "https://keras.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
