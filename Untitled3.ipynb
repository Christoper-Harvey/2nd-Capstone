{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready !!\n"
     ]
    }
   ],
   "source": [
    "target = pd.read_csv('target.csv')\n",
    "data = pd.read_csv('data.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "vdata = pd.read_csv('vdata.csv')\n",
    "vtarget = pd.read_csv('vtarget.csv')\n",
    "btarget = pd.read_csv('btarget.csv', header=None)\n",
    "bvtarget = pd.read_csv('bvtarget.csv', header=None)\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "print('Data is ready !!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358882</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358883</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358884</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358885</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358886</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358887</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358888</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358889</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358890</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358891</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358892</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358893</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358894</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358895</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358896</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358897</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358898</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358899</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358900</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358901</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358902</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358903</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358904</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358905</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358906</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358907</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358908</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358909</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358910</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358911</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>358912 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "5       0\n",
       "6       0\n",
       "7       0\n",
       "8       0\n",
       "9       0\n",
       "10      0\n",
       "11      0\n",
       "12      0\n",
       "13      0\n",
       "14      0\n",
       "15      0\n",
       "16      0\n",
       "17      0\n",
       "18      0\n",
       "19      0\n",
       "20      0\n",
       "21      0\n",
       "22      0\n",
       "23      0\n",
       "24      0\n",
       "25      0\n",
       "26      0\n",
       "27      0\n",
       "28      0\n",
       "29      0\n",
       "...    ..\n",
       "358882  4\n",
       "358883  4\n",
       "358884  4\n",
       "358885  4\n",
       "358886  4\n",
       "358887  4\n",
       "358888  4\n",
       "358889  4\n",
       "358890  4\n",
       "358891  4\n",
       "358892  4\n",
       "358893  4\n",
       "358894  4\n",
       "358895  4\n",
       "358896  4\n",
       "358897  4\n",
       "358898  4\n",
       "358899  4\n",
       "358900  4\n",
       "358901  4\n",
       "358902  4\n",
       "358903  4\n",
       "358904  4\n",
       "358905  4\n",
       "358906  4\n",
       "358907  4\n",
       "358908  4\n",
       "358909  4\n",
       "358910  4\n",
       "358911  4\n",
       "\n",
       "[358912 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.values\n",
    "vdata = vdata.values\n",
    "test = test.values\n",
    "target = target['surface']\n",
    "vtarget = vtarget['surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2804,)\n",
      "(1006,)\n",
      "(358912, 1)\n",
      "(128768, 1)\n",
      "(128768, 23)\n",
      "(358912, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(488448, 23)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(target.shape)\n",
    "print(vtarget.shape)\n",
    "print(btarget.shape)\n",
    "print(bvtarget.shape)\n",
    "print(vdata.shape)\n",
    "print(data.shape)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape((358912, 23))\n",
    "vdata = vdata.reshape((128768, 23))\n",
    "test = test.reshape((488448, 23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# data = data.reshape((2804, 128, 23))\n",
    "# vdata = vdata.reshape((1006, 128, 23))\n",
    "# test = test.reshape((3816, 128, 23))\n",
    "num_classes = 9\n",
    "\n",
    "# tanh, sigmoid\n",
    "\n",
    "# model.add(LSTM(128, use_bias=False, kernel_regularizer=regularizers.l2(0.001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('tanh'))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(128, input_dim=23, use_bias=False, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(128, use_bias=False, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128, use_bias=False, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128, use_bias=False, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(128, use_bias=False, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# loss='mearn_squared_error'\n",
    "# 'adam'\n",
    "# 'Nadam'\n",
    "# loss = kullback_leibler_divergence\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 358912 samples, validate on 128768 samples\n",
      "Epoch 1/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.4979 - acc: 0.8321 - val_loss: 3.3645 - val_acc: 0.4413\n",
      "Epoch 2/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4955 - acc: 0.8324 - val_loss: 3.3320 - val_acc: 0.4684\n",
      "Epoch 3/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4923 - acc: 0.8337 - val_loss: 3.4148 - val_acc: 0.4576\n",
      "Epoch 4/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4899 - acc: 0.8347 - val_loss: 3.5317 - val_acc: 0.4272\n",
      "Epoch 5/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4845 - acc: 0.8365 - val_loss: 3.3498 - val_acc: 0.4678\n",
      "Epoch 6/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4812 - acc: 0.8374 - val_loss: 3.4131 - val_acc: 0.4544\n",
      "Epoch 7/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4815 - acc: 0.8374 - val_loss: 3.2950 - val_acc: 0.4704\n",
      "Epoch 8/300\n",
      "358912/358912 [==============================] - 29s 80us/step - loss: 0.4801 - acc: 0.8375 - val_loss: 3.3599 - val_acc: 0.4618\n",
      "Epoch 9/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4767 - acc: 0.8392 - val_loss: 3.4697 - val_acc: 0.4437\n",
      "Epoch 10/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4737 - acc: 0.8400 - val_loss: 3.4336 - val_acc: 0.4210\n",
      "Epoch 11/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4719 - acc: 0.8412 - val_loss: 3.3829 - val_acc: 0.4562\n",
      "Epoch 12/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.4689 - acc: 0.8425 - val_loss: 3.5099 - val_acc: 0.4441\n",
      "Epoch 13/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4664 - acc: 0.8430 - val_loss: 3.2613 - val_acc: 0.4708\n",
      "Epoch 14/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4643 - acc: 0.8439 - val_loss: 3.4466 - val_acc: 0.4608\n",
      "Epoch 15/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4617 - acc: 0.8445 - val_loss: 3.2990 - val_acc: 0.4740\n",
      "Epoch 16/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4613 - acc: 0.8449 - val_loss: 3.4449 - val_acc: 0.4399\n",
      "Epoch 17/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4582 - acc: 0.8459 - val_loss: 3.4027 - val_acc: 0.4586\n",
      "Epoch 18/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4584 - acc: 0.8465 - val_loss: 3.4461 - val_acc: 0.4616\n",
      "Epoch 19/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4551 - acc: 0.8473 - val_loss: 3.4147 - val_acc: 0.4530\n",
      "Epoch 20/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4511 - acc: 0.8491 - val_loss: 3.4633 - val_acc: 0.4464\n",
      "Epoch 21/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4508 - acc: 0.8486 - val_loss: 3.3698 - val_acc: 0.4519\n",
      "Epoch 22/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4493 - acc: 0.8487 - val_loss: 3.5485 - val_acc: 0.4453\n",
      "Epoch 23/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4491 - acc: 0.8491 - val_loss: 3.5509 - val_acc: 0.4521\n",
      "Epoch 24/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4448 - acc: 0.8508 - val_loss: 3.5856 - val_acc: 0.4289\n",
      "Epoch 25/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4433 - acc: 0.8517 - val_loss: 3.5325 - val_acc: 0.4564\n",
      "Epoch 26/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4445 - acc: 0.8512 - val_loss: 3.5347 - val_acc: 0.4313\n",
      "Epoch 27/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4424 - acc: 0.8514 - val_loss: 3.4217 - val_acc: 0.4680\n",
      "Epoch 28/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4418 - acc: 0.8520 - val_loss: 3.3828 - val_acc: 0.4538\n",
      "Epoch 29/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4374 - acc: 0.8535 - val_loss: 3.3458 - val_acc: 0.4790\n",
      "Epoch 30/300\n",
      "358912/358912 [==============================] - 26s 72us/step - loss: 0.4393 - acc: 0.8534 - val_loss: 3.4246 - val_acc: 0.4502\n",
      "Epoch 31/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4404 - acc: 0.8526 - val_loss: 3.4946 - val_acc: 0.4572\n",
      "Epoch 32/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.4362 - acc: 0.8543 - val_loss: 3.4550 - val_acc: 0.4697\n",
      "Epoch 33/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4344 - acc: 0.8550 - val_loss: 3.4219 - val_acc: 0.4522\n",
      "Epoch 34/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4323 - acc: 0.8556 - val_loss: 3.4001 - val_acc: 0.4729\n",
      "Epoch 35/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.4322 - acc: 0.8557 - val_loss: 3.4249 - val_acc: 0.4668\n",
      "Epoch 36/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4303 - acc: 0.8559 - val_loss: 3.5978 - val_acc: 0.4340\n",
      "Epoch 37/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4295 - acc: 0.8568 - val_loss: 3.4309 - val_acc: 0.4430\n",
      "Epoch 38/300\n",
      "358912/358912 [==============================] - 29s 80us/step - loss: 0.4285 - acc: 0.8572 - val_loss: 3.4945 - val_acc: 0.4446\n",
      "Epoch 39/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4286 - acc: 0.8571 - val_loss: 3.5353 - val_acc: 0.4442\n",
      "Epoch 40/300\n",
      "358912/358912 [==============================] - 29s 80us/step - loss: 0.4274 - acc: 0.8575 - val_loss: 3.4610 - val_acc: 0.4727\n",
      "Epoch 41/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4285 - acc: 0.8568 - val_loss: 3.3654 - val_acc: 0.4896\n",
      "Epoch 42/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4226 - acc: 0.8592 - val_loss: 3.4568 - val_acc: 0.4562\n",
      "Epoch 43/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4232 - acc: 0.8589 - val_loss: 3.4721 - val_acc: 0.4518\n",
      "Epoch 44/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4197 - acc: 0.8604 - val_loss: 3.5956 - val_acc: 0.4285\n",
      "Epoch 45/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4232 - acc: 0.8590 - val_loss: 3.4667 - val_acc: 0.4772\n",
      "Epoch 46/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.4228 - acc: 0.8593 - val_loss: 3.4059 - val_acc: 0.4754\n",
      "Epoch 47/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4197 - acc: 0.8601 - val_loss: 3.4634 - val_acc: 0.4546\n",
      "Epoch 48/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4185 - acc: 0.8601 - val_loss: 3.4302 - val_acc: 0.4628\n",
      "Epoch 49/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4164 - acc: 0.8607 - val_loss: 3.6641 - val_acc: 0.4338\n",
      "Epoch 50/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4171 - acc: 0.8614 - val_loss: 3.4964 - val_acc: 0.4632\n",
      "Epoch 51/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4156 - acc: 0.8616 - val_loss: 3.6250 - val_acc: 0.4327\n",
      "Epoch 52/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4152 - acc: 0.8616 - val_loss: 3.6120 - val_acc: 0.4338\n",
      "Epoch 53/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.4137 - acc: 0.8617 - val_loss: 3.6373 - val_acc: 0.4437\n",
      "Epoch 54/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4119 - acc: 0.8631 - val_loss: 3.3364 - val_acc: 0.4924\n",
      "Epoch 55/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4120 - acc: 0.8635 - val_loss: 3.4612 - val_acc: 0.4704\n",
      "Epoch 56/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4083 - acc: 0.8640 - val_loss: 3.4864 - val_acc: 0.4671\n",
      "Epoch 57/300\n",
      "358912/358912 [==============================] - 29s 80us/step - loss: 0.4103 - acc: 0.8636 - val_loss: 3.4421 - val_acc: 0.4644\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4076 - acc: 0.8642 - val_loss: 3.5489 - val_acc: 0.4610\n",
      "Epoch 59/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4067 - acc: 0.8649 - val_loss: 3.4258 - val_acc: 0.4720\n",
      "Epoch 60/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4068 - acc: 0.8647 - val_loss: 3.6572 - val_acc: 0.4379\n",
      "Epoch 61/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4070 - acc: 0.8648 - val_loss: 3.5164 - val_acc: 0.4609\n",
      "Epoch 62/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4062 - acc: 0.8650 - val_loss: 3.6503 - val_acc: 0.4335\n",
      "Epoch 63/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4067 - acc: 0.8652 - val_loss: 3.4907 - val_acc: 0.4663\n",
      "Epoch 64/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4063 - acc: 0.8650 - val_loss: 3.4536 - val_acc: 0.4719\n",
      "Epoch 65/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4051 - acc: 0.8656 - val_loss: 3.7318 - val_acc: 0.4296\n",
      "Epoch 66/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4032 - acc: 0.8663 - val_loss: 3.5188 - val_acc: 0.4602\n",
      "Epoch 67/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4035 - acc: 0.8659 - val_loss: 3.7301 - val_acc: 0.4313\n",
      "Epoch 68/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4030 - acc: 0.8665 - val_loss: 3.6243 - val_acc: 0.4394\n",
      "Epoch 69/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4025 - acc: 0.8666 - val_loss: 3.6531 - val_acc: 0.4552\n",
      "Epoch 70/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.4033 - acc: 0.8669 - val_loss: 3.4397 - val_acc: 0.4707\n",
      "Epoch 71/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3986 - acc: 0.8681 - val_loss: 3.5278 - val_acc: 0.4724\n",
      "Epoch 72/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3981 - acc: 0.8674 - val_loss: 3.6542 - val_acc: 0.4488\n",
      "Epoch 73/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3987 - acc: 0.8678 - val_loss: 3.5890 - val_acc: 0.4439\n",
      "Epoch 74/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3980 - acc: 0.8683 - val_loss: 3.6359 - val_acc: 0.4547\n",
      "Epoch 75/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3982 - acc: 0.8683 - val_loss: 3.5454 - val_acc: 0.4756\n",
      "Epoch 76/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3955 - acc: 0.8684 - val_loss: 3.6476 - val_acc: 0.4644\n",
      "Epoch 77/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3956 - acc: 0.8690 - val_loss: 3.7259 - val_acc: 0.4381\n",
      "Epoch 78/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3978 - acc: 0.8679 - val_loss: 3.6604 - val_acc: 0.4496\n",
      "Epoch 79/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3945 - acc: 0.8695 - val_loss: 3.4988 - val_acc: 0.4774\n",
      "Epoch 80/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3930 - acc: 0.8700 - val_loss: 3.5139 - val_acc: 0.4763\n",
      "Epoch 81/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3920 - acc: 0.8698 - val_loss: 3.7632 - val_acc: 0.4218\n",
      "Epoch 82/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.3935 - acc: 0.8694 - val_loss: 3.4628 - val_acc: 0.4789\n",
      "Epoch 83/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3938 - acc: 0.8699 - val_loss: 3.6591 - val_acc: 0.4305\n",
      "Epoch 84/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3912 - acc: 0.8709 - val_loss: 3.5637 - val_acc: 0.4665\n",
      "Epoch 85/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3938 - acc: 0.8695 - val_loss: 3.5936 - val_acc: 0.4564\n",
      "Epoch 86/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3896 - acc: 0.8708 - val_loss: 3.5260 - val_acc: 0.4754\n",
      "Epoch 87/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3914 - acc: 0.8708 - val_loss: 3.6814 - val_acc: 0.4470\n",
      "Epoch 88/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3903 - acc: 0.8714 - val_loss: 3.5146 - val_acc: 0.4712\n",
      "Epoch 89/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3896 - acc: 0.8713 - val_loss: 3.5557 - val_acc: 0.4778\n",
      "Epoch 90/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3872 - acc: 0.8719 - val_loss: 3.5601 - val_acc: 0.4600\n",
      "Epoch 91/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3891 - acc: 0.8715 - val_loss: 3.7221 - val_acc: 0.4416\n",
      "Epoch 92/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3879 - acc: 0.8715 - val_loss: 3.6125 - val_acc: 0.4559\n",
      "Epoch 93/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3883 - acc: 0.8715 - val_loss: 3.6551 - val_acc: 0.4501\n",
      "Epoch 94/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3865 - acc: 0.8729 - val_loss: 3.5213 - val_acc: 0.4713\n",
      "Epoch 95/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3864 - acc: 0.8720 - val_loss: 3.6768 - val_acc: 0.4502\n",
      "Epoch 96/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3862 - acc: 0.8721 - val_loss: 3.6344 - val_acc: 0.4588\n",
      "Epoch 97/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3887 - acc: 0.8715 - val_loss: 3.5700 - val_acc: 0.4608\n",
      "Epoch 98/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3849 - acc: 0.8727 - val_loss: 3.6792 - val_acc: 0.4578\n",
      "Epoch 99/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3864 - acc: 0.8724 - val_loss: 3.5430 - val_acc: 0.4731\n",
      "Epoch 100/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3857 - acc: 0.8726 - val_loss: 3.5721 - val_acc: 0.4778\n",
      "Epoch 101/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3847 - acc: 0.8729 - val_loss: 3.4950 - val_acc: 0.4696\n",
      "Epoch 102/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3804 - acc: 0.8742 - val_loss: 3.5767 - val_acc: 0.4637\n",
      "Epoch 103/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3793 - acc: 0.8743 - val_loss: 3.6997 - val_acc: 0.4548\n",
      "Epoch 104/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3810 - acc: 0.8745 - val_loss: 3.6429 - val_acc: 0.4513\n",
      "Epoch 105/300\n",
      "358912/358912 [==============================] - 29s 80us/step - loss: 0.3768 - acc: 0.8757 - val_loss: 3.6601 - val_acc: 0.4491\n",
      "Epoch 106/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3762 - acc: 0.8756 - val_loss: 3.8843 - val_acc: 0.4277\n",
      "Epoch 107/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3764 - acc: 0.8756 - val_loss: 3.7102 - val_acc: 0.4497\n",
      "Epoch 108/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3750 - acc: 0.8767 - val_loss: 3.6602 - val_acc: 0.4544\n",
      "Epoch 109/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3734 - acc: 0.8766 - val_loss: 3.6947 - val_acc: 0.4452\n",
      "Epoch 110/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3701 - acc: 0.8778 - val_loss: 3.6779 - val_acc: 0.4561\n",
      "Epoch 111/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3724 - acc: 0.8775 - val_loss: 3.8615 - val_acc: 0.4149\n",
      "Epoch 112/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3719 - acc: 0.8768 - val_loss: 3.6718 - val_acc: 0.4572\n",
      "Epoch 113/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3708 - acc: 0.8783 - val_loss: 3.8925 - val_acc: 0.4300\n",
      "Epoch 114/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3716 - acc: 0.8779 - val_loss: 3.8132 - val_acc: 0.4435\n",
      "Epoch 115/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3710 - acc: 0.8780 - val_loss: 3.5777 - val_acc: 0.4787\n",
      "Epoch 116/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3709 - acc: 0.8780 - val_loss: 3.7146 - val_acc: 0.4575\n",
      "Epoch 117/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3675 - acc: 0.8791 - val_loss: 3.6840 - val_acc: 0.4550\n",
      "Epoch 118/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3690 - acc: 0.8786 - val_loss: 3.7982 - val_acc: 0.4262\n",
      "Epoch 119/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3684 - acc: 0.8788 - val_loss: 3.7949 - val_acc: 0.4277\n",
      "Epoch 120/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3678 - acc: 0.8787 - val_loss: 3.7638 - val_acc: 0.4430\n",
      "Epoch 121/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3653 - acc: 0.8797 - val_loss: 3.6482 - val_acc: 0.4697\n",
      "Epoch 122/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3670 - acc: 0.8788 - val_loss: 3.6307 - val_acc: 0.4638\n",
      "Epoch 123/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3663 - acc: 0.8795 - val_loss: 3.7255 - val_acc: 0.4624\n",
      "Epoch 124/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3671 - acc: 0.8790 - val_loss: 3.6700 - val_acc: 0.4649\n",
      "Epoch 125/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3644 - acc: 0.8803 - val_loss: 3.8056 - val_acc: 0.4256\n",
      "Epoch 126/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3645 - acc: 0.8798 - val_loss: 3.7042 - val_acc: 0.4599\n",
      "Epoch 127/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3646 - acc: 0.8794 - val_loss: 3.6825 - val_acc: 0.4500\n",
      "Epoch 128/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3645 - acc: 0.8801 - val_loss: 3.9391 - val_acc: 0.4255\n",
      "Epoch 129/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3642 - acc: 0.8801 - val_loss: 3.6884 - val_acc: 0.4910\n",
      "Epoch 130/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3623 - acc: 0.8808 - val_loss: 3.8039 - val_acc: 0.4379\n",
      "Epoch 131/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3631 - acc: 0.8809 - val_loss: 3.8518 - val_acc: 0.4363\n",
      "Epoch 132/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3623 - acc: 0.8815 - val_loss: 3.7720 - val_acc: 0.4488\n",
      "Epoch 133/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3633 - acc: 0.8802 - val_loss: 3.7469 - val_acc: 0.4643\n",
      "Epoch 134/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3605 - acc: 0.8818 - val_loss: 3.7673 - val_acc: 0.4472\n",
      "Epoch 135/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3599 - acc: 0.8819 - val_loss: 3.6500 - val_acc: 0.4751\n",
      "Epoch 136/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3600 - acc: 0.8812 - val_loss: 3.8523 - val_acc: 0.4517\n",
      "Epoch 137/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3592 - acc: 0.8821 - val_loss: 3.8700 - val_acc: 0.4530\n",
      "Epoch 138/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3596 - acc: 0.8819 - val_loss: 3.7176 - val_acc: 0.4616\n",
      "Epoch 139/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3590 - acc: 0.8819 - val_loss: 3.7164 - val_acc: 0.4561\n",
      "Epoch 140/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3582 - acc: 0.8820 - val_loss: 3.7079 - val_acc: 0.4610\n",
      "Epoch 141/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3579 - acc: 0.8823 - val_loss: 3.6292 - val_acc: 0.4805\n",
      "Epoch 142/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3582 - acc: 0.8821 - val_loss: 3.5909 - val_acc: 0.4786\n",
      "Epoch 143/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3568 - acc: 0.8829 - val_loss: 3.9208 - val_acc: 0.4322\n",
      "Epoch 144/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3561 - acc: 0.8831 - val_loss: 3.8639 - val_acc: 0.4406\n",
      "Epoch 145/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3571 - acc: 0.8828 - val_loss: 3.6162 - val_acc: 0.4759\n",
      "Epoch 146/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3564 - acc: 0.8828 - val_loss: 3.8067 - val_acc: 0.4517\n",
      "Epoch 147/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3544 - acc: 0.8838 - val_loss: 3.8261 - val_acc: 0.4445\n",
      "Epoch 148/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3570 - acc: 0.8831 - val_loss: 3.5610 - val_acc: 0.4740\n",
      "Epoch 149/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3555 - acc: 0.8831 - val_loss: 4.1393 - val_acc: 0.4110\n",
      "Epoch 150/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3556 - acc: 0.8830 - val_loss: 3.6490 - val_acc: 0.4761\n",
      "Epoch 151/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3550 - acc: 0.8837 - val_loss: 3.6193 - val_acc: 0.4796\n",
      "Epoch 152/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3542 - acc: 0.8839 - val_loss: 3.6733 - val_acc: 0.4712\n",
      "Epoch 153/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3540 - acc: 0.8841 - val_loss: 3.8657 - val_acc: 0.4449\n",
      "Epoch 154/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3531 - acc: 0.8839 - val_loss: 3.7409 - val_acc: 0.4621\n",
      "Epoch 155/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3546 - acc: 0.8841 - val_loss: 3.9344 - val_acc: 0.4226\n",
      "Epoch 156/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3555 - acc: 0.8830 - val_loss: 3.6461 - val_acc: 0.4737\n",
      "Epoch 157/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3554 - acc: 0.8832 - val_loss: 3.8039 - val_acc: 0.4482\n",
      "Epoch 158/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3513 - acc: 0.8851 - val_loss: 3.7029 - val_acc: 0.4649\n",
      "Epoch 159/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3506 - acc: 0.8852 - val_loss: 3.7915 - val_acc: 0.4523\n",
      "Epoch 160/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3499 - acc: 0.8854 - val_loss: 3.8534 - val_acc: 0.4358\n",
      "Epoch 161/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3535 - acc: 0.8841 - val_loss: 3.7033 - val_acc: 0.4730\n",
      "Epoch 162/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3519 - acc: 0.8848 - val_loss: 3.7641 - val_acc: 0.4591\n",
      "Epoch 163/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3531 - acc: 0.8842 - val_loss: 3.7129 - val_acc: 0.4665\n",
      "Epoch 164/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3496 - acc: 0.8855 - val_loss: 3.6564 - val_acc: 0.4642\n",
      "Epoch 165/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3496 - acc: 0.8851 - val_loss: 3.8388 - val_acc: 0.4351\n",
      "Epoch 166/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3504 - acc: 0.8852 - val_loss: 4.1533 - val_acc: 0.4071\n",
      "Epoch 167/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3486 - acc: 0.8853 - val_loss: 3.6951 - val_acc: 0.4602\n",
      "Epoch 168/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3497 - acc: 0.8856 - val_loss: 3.7210 - val_acc: 0.4739\n",
      "Epoch 169/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3483 - acc: 0.8858 - val_loss: 3.8311 - val_acc: 0.4558\n",
      "Epoch 170/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3482 - acc: 0.8858 - val_loss: 3.8484 - val_acc: 0.4530\n",
      "Epoch 171/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3480 - acc: 0.8861 - val_loss: 3.7460 - val_acc: 0.4670\n",
      "Epoch 172/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3481 - acc: 0.8862 - val_loss: 3.7926 - val_acc: 0.4592\n",
      "Epoch 173/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3496 - acc: 0.8854 - val_loss: 3.7753 - val_acc: 0.4499\n",
      "Epoch 174/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3479 - acc: 0.8859 - val_loss: 3.6907 - val_acc: 0.4675\n",
      "Epoch 175/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3460 - acc: 0.8865 - val_loss: 3.7235 - val_acc: 0.4718\n",
      "Epoch 176/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3458 - acc: 0.8867 - val_loss: 3.6495 - val_acc: 0.4787\n",
      "Epoch 177/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.3459 - acc: 0.8864 - val_loss: 3.7263 - val_acc: 0.4739\n",
      "Epoch 178/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3454 - acc: 0.8864 - val_loss: 3.7866 - val_acc: 0.4569\n",
      "Epoch 179/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3445 - acc: 0.8869 - val_loss: 3.7189 - val_acc: 0.4664\n",
      "Epoch 180/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3458 - acc: 0.8861 - val_loss: 3.8213 - val_acc: 0.4499\n",
      "Epoch 181/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3471 - acc: 0.8862 - val_loss: 3.7318 - val_acc: 0.4650\n",
      "Epoch 182/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3450 - acc: 0.8866 - val_loss: 3.9139 - val_acc: 0.4511\n",
      "Epoch 183/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3467 - acc: 0.8861 - val_loss: 3.8662 - val_acc: 0.4521\n",
      "Epoch 184/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3450 - acc: 0.8871 - val_loss: 3.8622 - val_acc: 0.4357\n",
      "Epoch 185/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3434 - acc: 0.8874 - val_loss: 3.7179 - val_acc: 0.4688\n",
      "Epoch 186/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3440 - acc: 0.8870 - val_loss: 3.8189 - val_acc: 0.4444\n",
      "Epoch 187/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3409 - acc: 0.8884 - val_loss: 3.9612 - val_acc: 0.4451\n",
      "Epoch 188/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3444 - acc: 0.8870 - val_loss: 3.6648 - val_acc: 0.4723\n",
      "Epoch 189/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3448 - acc: 0.8875 - val_loss: 4.0035 - val_acc: 0.4296\n",
      "Epoch 190/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3421 - acc: 0.8878 - val_loss: 3.8859 - val_acc: 0.4494\n",
      "Epoch 191/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3453 - acc: 0.8870 - val_loss: 3.7539 - val_acc: 0.4716\n",
      "Epoch 192/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3431 - acc: 0.8874 - val_loss: 3.9765 - val_acc: 0.4338\n",
      "Epoch 193/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3437 - acc: 0.8875 - val_loss: 3.7536 - val_acc: 0.4641\n",
      "Epoch 194/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3418 - acc: 0.8881 - val_loss: 3.9516 - val_acc: 0.4424\n",
      "Epoch 195/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3422 - acc: 0.8882 - val_loss: 3.7496 - val_acc: 0.4720\n",
      "Epoch 196/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3421 - acc: 0.8880 - val_loss: 3.8305 - val_acc: 0.4545\n",
      "Epoch 197/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3409 - acc: 0.8886 - val_loss: 3.8291 - val_acc: 0.4630\n",
      "Epoch 198/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3408 - acc: 0.8886 - val_loss: 3.7759 - val_acc: 0.4578\n",
      "Epoch 199/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3423 - acc: 0.8881 - val_loss: 3.8813 - val_acc: 0.4440\n",
      "Epoch 200/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3418 - acc: 0.8884 - val_loss: 3.9499 - val_acc: 0.4345\n",
      "Epoch 201/300\n",
      "358912/358912 [==============================] - 29s 80us/step - loss: 0.3409 - acc: 0.8880 - val_loss: 3.7172 - val_acc: 0.4663\n",
      "Epoch 202/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3430 - acc: 0.8878 - val_loss: 3.8299 - val_acc: 0.4640\n",
      "Epoch 203/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3394 - acc: 0.8890 - val_loss: 3.9452 - val_acc: 0.4473\n",
      "Epoch 204/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3406 - acc: 0.8880 - val_loss: 3.7445 - val_acc: 0.4743\n",
      "Epoch 205/300\n",
      "358912/358912 [==============================] - 29s 80us/step - loss: 0.3421 - acc: 0.8880 - val_loss: 3.7218 - val_acc: 0.4769\n",
      "Epoch 206/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3390 - acc: 0.8891 - val_loss: 3.8665 - val_acc: 0.4504\n",
      "Epoch 207/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3400 - acc: 0.8886 - val_loss: 3.7731 - val_acc: 0.4680\n",
      "Epoch 208/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3395 - acc: 0.8890 - val_loss: 3.7623 - val_acc: 0.4687\n",
      "Epoch 209/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3376 - acc: 0.8895 - val_loss: 3.9197 - val_acc: 0.4480\n",
      "Epoch 210/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3397 - acc: 0.8888 - val_loss: 3.9882 - val_acc: 0.4402\n",
      "Epoch 211/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3397 - acc: 0.8888 - val_loss: 3.9251 - val_acc: 0.4411\n",
      "Epoch 212/300\n",
      "358912/358912 [==============================] - 29s 80us/step - loss: 0.3394 - acc: 0.8887 - val_loss: 3.8933 - val_acc: 0.4363\n",
      "Epoch 213/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3380 - acc: 0.8892 - val_loss: 3.7967 - val_acc: 0.4639\n",
      "Epoch 214/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3389 - acc: 0.8894 - val_loss: 3.8925 - val_acc: 0.4388\n",
      "Epoch 215/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3385 - acc: 0.8890 - val_loss: 3.8430 - val_acc: 0.4538\n",
      "Epoch 216/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3396 - acc: 0.8885 - val_loss: 3.8385 - val_acc: 0.4548\n",
      "Epoch 217/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3380 - acc: 0.8897 - val_loss: 4.0434 - val_acc: 0.4181\n",
      "Epoch 218/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3374 - acc: 0.8897 - val_loss: 3.7932 - val_acc: 0.4596\n",
      "Epoch 219/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3376 - acc: 0.8896 - val_loss: 3.8067 - val_acc: 0.4616\n",
      "Epoch 220/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3384 - acc: 0.8895 - val_loss: 3.8386 - val_acc: 0.4623\n",
      "Epoch 221/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3372 - acc: 0.8892 - val_loss: 4.1196 - val_acc: 0.4266\n",
      "Epoch 222/300\n",
      "358912/358912 [==============================] - 29s 80us/step - loss: 0.3371 - acc: 0.8895 - val_loss: 3.9395 - val_acc: 0.4535\n",
      "Epoch 223/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3363 - acc: 0.8901 - val_loss: 3.9441 - val_acc: 0.4498\n",
      "Epoch 224/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3352 - acc: 0.8907 - val_loss: 3.7518 - val_acc: 0.4769\n",
      "Epoch 225/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3380 - acc: 0.8893 - val_loss: 3.7117 - val_acc: 0.4717\n",
      "Epoch 226/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3340 - acc: 0.8907 - val_loss: 3.9589 - val_acc: 0.4373\n",
      "Epoch 227/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3343 - acc: 0.8909 - val_loss: 3.8194 - val_acc: 0.4466\n",
      "Epoch 228/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3346 - acc: 0.8906 - val_loss: 4.0640 - val_acc: 0.4195\n",
      "Epoch 229/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3366 - acc: 0.8906 - val_loss: 4.1008 - val_acc: 0.4091\n",
      "Epoch 230/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3332 - acc: 0.8908 - val_loss: 3.7678 - val_acc: 0.4641\n",
      "Epoch 231/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3346 - acc: 0.8905 - val_loss: 3.8827 - val_acc: 0.4508\n",
      "Epoch 232/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3355 - acc: 0.8901 - val_loss: 3.7805 - val_acc: 0.4540\n",
      "Epoch 233/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3331 - acc: 0.8915 - val_loss: 3.7816 - val_acc: 0.4676\n",
      "Epoch 234/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3342 - acc: 0.8908 - val_loss: 3.7946 - val_acc: 0.4722\n",
      "Epoch 235/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3327 - acc: 0.8914 - val_loss: 3.8898 - val_acc: 0.4484\n",
      "Epoch 236/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3357 - acc: 0.8902 - val_loss: 3.8827 - val_acc: 0.4556\n",
      "Epoch 237/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3338 - acc: 0.8908 - val_loss: 4.0115 - val_acc: 0.4410\n",
      "Epoch 238/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3339 - acc: 0.8906 - val_loss: 3.8646 - val_acc: 0.4630\n",
      "Epoch 239/300\n",
      "358912/358912 [==============================] - 30s 82us/step - loss: 0.3330 - acc: 0.8912 - val_loss: 3.7787 - val_acc: 0.4736\n",
      "Epoch 240/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3330 - acc: 0.8912 - val_loss: 3.9542 - val_acc: 0.4520\n",
      "Epoch 241/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3334 - acc: 0.8905 - val_loss: 3.7996 - val_acc: 0.4630\n",
      "Epoch 242/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.3336 - acc: 0.8904 - val_loss: 4.1005 - val_acc: 0.4075\n",
      "Epoch 243/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.3305 - acc: 0.8921 - val_loss: 3.8231 - val_acc: 0.4681\n",
      "Epoch 244/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3337 - acc: 0.8904 - val_loss: 3.9559 - val_acc: 0.4467\n",
      "Epoch 245/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.3310 - acc: 0.8925 - val_loss: 3.7727 - val_acc: 0.4581\n",
      "Epoch 246/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.3308 - acc: 0.8920 - val_loss: 4.0366 - val_acc: 0.4253\n",
      "Epoch 247/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3322 - acc: 0.8918 - val_loss: 3.7258 - val_acc: 0.4715\n",
      "Epoch 248/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3310 - acc: 0.8927 - val_loss: 3.7949 - val_acc: 0.4695\n",
      "Epoch 249/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3325 - acc: 0.8910 - val_loss: 3.8324 - val_acc: 0.4688\n",
      "Epoch 250/300\n",
      "358912/358912 [==============================] - 29s 82us/step - loss: 0.3315 - acc: 0.8918 - val_loss: 3.9797 - val_acc: 0.4429\n",
      "Epoch 251/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3310 - acc: 0.8917 - val_loss: 3.9400 - val_acc: 0.4445\n",
      "Epoch 252/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3315 - acc: 0.8918 - val_loss: 3.8401 - val_acc: 0.4564\n",
      "Epoch 253/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3299 - acc: 0.8918 - val_loss: 3.8809 - val_acc: 0.4519\n",
      "Epoch 254/300\n",
      "358912/358912 [==============================] - 30s 82us/step - loss: 0.3296 - acc: 0.8917 - val_loss: 4.0479 - val_acc: 0.4286\n",
      "Epoch 255/300\n",
      "358912/358912 [==============================] - 30s 83us/step - loss: 0.3301 - acc: 0.8923 - val_loss: 3.8417 - val_acc: 0.4730\n",
      "Epoch 256/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3302 - acc: 0.8920 - val_loss: 3.9772 - val_acc: 0.4417\n",
      "Epoch 257/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3309 - acc: 0.8916 - val_loss: 3.9255 - val_acc: 0.4546\n",
      "Epoch 258/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3307 - acc: 0.8915 - val_loss: 3.7333 - val_acc: 0.4767\n",
      "Epoch 259/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3298 - acc: 0.8922 - val_loss: 3.9253 - val_acc: 0.4499\n",
      "Epoch 260/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3267 - acc: 0.8934 - val_loss: 3.8554 - val_acc: 0.4647\n",
      "Epoch 261/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3294 - acc: 0.8922 - val_loss: 4.0877 - val_acc: 0.4269\n",
      "Epoch 262/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3295 - acc: 0.8916 - val_loss: 3.8777 - val_acc: 0.4628\n",
      "Epoch 263/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3293 - acc: 0.8919 - val_loss: 3.8783 - val_acc: 0.4590\n",
      "Epoch 264/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3288 - acc: 0.8922 - val_loss: 3.9031 - val_acc: 0.4561\n",
      "Epoch 265/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3288 - acc: 0.8927 - val_loss: 3.9070 - val_acc: 0.4614\n",
      "Epoch 266/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3279 - acc: 0.8929 - val_loss: 3.9276 - val_acc: 0.4526\n",
      "Epoch 267/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3269 - acc: 0.8930 - val_loss: 3.8965 - val_acc: 0.4584\n",
      "Epoch 268/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3282 - acc: 0.8928 - val_loss: 3.8516 - val_acc: 0.4673\n",
      "Epoch 269/300\n",
      "358912/358912 [==============================] - 29s 80us/step - loss: 0.3255 - acc: 0.8940 - val_loss: 3.9299 - val_acc: 0.4606\n",
      "Epoch 270/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3272 - acc: 0.8932 - val_loss: 3.9076 - val_acc: 0.4628\n",
      "Epoch 271/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3291 - acc: 0.8919 - val_loss: 3.9332 - val_acc: 0.4631\n",
      "Epoch 272/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3307 - acc: 0.8915 - val_loss: 3.9680 - val_acc: 0.4401\n",
      "Epoch 273/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3269 - acc: 0.8932 - val_loss: 3.9238 - val_acc: 0.4541\n",
      "Epoch 274/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3255 - acc: 0.8939 - val_loss: 3.8620 - val_acc: 0.4739\n",
      "Epoch 275/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3278 - acc: 0.8928 - val_loss: 3.9615 - val_acc: 0.4505\n",
      "Epoch 276/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3261 - acc: 0.8937 - val_loss: 3.8343 - val_acc: 0.4594\n",
      "Epoch 277/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3277 - acc: 0.8928 - val_loss: 3.9784 - val_acc: 0.4409\n",
      "Epoch 278/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3266 - acc: 0.8938 - val_loss: 4.0412 - val_acc: 0.4311\n",
      "Epoch 279/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3288 - acc: 0.8926 - val_loss: 3.9853 - val_acc: 0.4502\n",
      "Epoch 280/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3258 - acc: 0.8939 - val_loss: 3.9085 - val_acc: 0.4705\n",
      "Epoch 281/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3273 - acc: 0.8932 - val_loss: 3.7866 - val_acc: 0.4666\n",
      "Epoch 282/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3244 - acc: 0.8943 - val_loss: 3.8970 - val_acc: 0.4603\n",
      "Epoch 283/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3264 - acc: 0.8934 - val_loss: 3.9981 - val_acc: 0.4481\n",
      "Epoch 284/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3257 - acc: 0.8934 - val_loss: 3.8658 - val_acc: 0.4626\n",
      "Epoch 285/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3259 - acc: 0.8940 - val_loss: 3.8516 - val_acc: 0.4539\n",
      "Epoch 286/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3248 - acc: 0.8937 - val_loss: 3.8652 - val_acc: 0.4697\n",
      "Epoch 287/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3265 - acc: 0.8937 - val_loss: 3.7664 - val_acc: 0.4729\n",
      "Epoch 288/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3234 - acc: 0.8940 - val_loss: 3.8436 - val_acc: 0.4587\n",
      "Epoch 289/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3246 - acc: 0.8941 - val_loss: 4.1118 - val_acc: 0.4261\n",
      "Epoch 290/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3252 - acc: 0.8943 - val_loss: 3.9253 - val_acc: 0.4529\n",
      "Epoch 291/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3269 - acc: 0.8926 - val_loss: 3.9633 - val_acc: 0.4486\n",
      "Epoch 292/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3228 - acc: 0.8942 - val_loss: 3.8391 - val_acc: 0.4729\n",
      "Epoch 293/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3244 - acc: 0.8941 - val_loss: 3.8741 - val_acc: 0.4571\n",
      "Epoch 294/300\n",
      "358912/358912 [==============================] - 29s 80us/step - loss: 0.3241 - acc: 0.8944 - val_loss: 3.9659 - val_acc: 0.4519\n",
      "Epoch 295/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3239 - acc: 0.8942 - val_loss: 3.9456 - val_acc: 0.4543\n",
      "Epoch 296/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3234 - acc: 0.8946 - val_loss: 3.8811 - val_acc: 0.4664\n",
      "Epoch 297/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3238 - acc: 0.8942 - val_loss: 3.8016 - val_acc: 0.4617\n",
      "Epoch 298/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3232 - acc: 0.8948 - val_loss: 4.1366 - val_acc: 0.4170\n",
      "Epoch 299/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3228 - acc: 0.8948 - val_loss: 3.9479 - val_acc: 0.4567\n",
      "Epoch 300/300\n",
      "358912/358912 [==============================] - 29s 81us/step - loss: 0.3235 - acc: 0.8943 - val_loss: 3.8228 - val_acc: 0.4630\n"
     ]
    }
   ],
   "source": [
    "ah = model.fit(data, btarget,\n",
    "              batch_size=128,\n",
    "              epochs=300,\n",
    "              validation_data=(vdata, bvtarget),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dd4df34a90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decode_dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-a0adf4d02d06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'surface'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'surface'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'surface'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_dic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# sub.head(15)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decode_dic' is not defined"
     ]
    }
   ],
   "source": [
    "preds = model.predict_classes(test)\n",
    "preds = preds[0:-1:128]\n",
    "sub['surface'] = preds\n",
    "sub['surface'] = sub['surface'].map(decode_dic)\n",
    "# sub.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission12.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
