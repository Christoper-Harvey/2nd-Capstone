{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages are ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Our standard packages for data science.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "# Our main packages for standard machine learning\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from numba import jit\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "print('Packages are ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "# Lets import our data\n",
    "target = pd.read_csv('target.csv')\n",
    "data = pd.read_csv('data.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "vdata = pd.read_csv('vdata.csv')\n",
    "vtarget = pd.read_csv('vtarget.csv')\n",
    "ltarget = pd.read_csv('ltarget.csv', header=None)\n",
    "lvtarget = pd.read_csv('lvtarget.csv', header=None)\n",
    "wdata = pd.read_csv('whole_data.csv')\n",
    "wtarget = pd.read_csv('whole_target.csv', header=None)\n",
    "\n",
    "winedata = pd.read_csv('winedata.csv')\n",
    "\n",
    "ccdata = pd.read_csv('creditcard.csv')\n",
    "\n",
    "irisdata = pd.read_csv('irisdata.csv')\n",
    "iristarget = pd.read_csv('iristarget.csv')\n",
    "\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets go ahead and set up our data. First lets make our target and drop it from wine.\n",
    "winetarget = winedata['quality']\n",
    "winedata = winedata.drop('quality', axis=1)\n",
    "\n",
    "# Lets trim off time and amount from ccdata as those are independent features we don't want the model to learn.\n",
    "cctarget = ccdata['Class']\n",
    "ccdata = ccdata.drop(['Time','Amount', 'Class'], axis=1)\n",
    "\n",
    "iristarget = iristarget['target']\n",
    "\n",
    "target = target['surface']\n",
    "vtarget = vtarget['surface']\n",
    "ltarget = ltarget[0]\n",
    "lvtarget = lvtarget[0]\n",
    "wdata = wdata.drop(['series_id', 'group_id', 'surface'], axis=1)\n",
    "wtarget = wtarget[0]\n",
    "\n",
    "irisd, id_test, irist, it_test = train_test_split(irisdata, iristarget, test_size=0.33, random_state=42)\n",
    "wined, wd_test, winet, wt_test = train_test_split(winedata, winetarget, test_size=0.33, random_state=42)\n",
    "ccd, ccd_test, cct, cct_test = train_test_split(ccdata, cctarget, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "(6497, 11)\n",
      "(6497,)\n",
      "(284807, 28)\n",
      "(284807,)\n",
      "(2804,)\n",
      "(1006,)\n",
      "(358912,)\n",
      "(128768,)\n",
      "(487680,)\n",
      "(487680, 23)\n",
      "(128768, 23)\n",
      "(358912, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(488448, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets make sure our train and target variables are even for every dataset\n",
    "print(irisdata.shape)\n",
    "print(iristarget.shape)\n",
    "\n",
    "print(winedata.shape)\n",
    "print(winetarget.shape)\n",
    "\n",
    "print(ccdata.shape)\n",
    "print(cctarget.shape)\n",
    "\n",
    "print(target.shape)\n",
    "print(vtarget.shape)\n",
    "print(ltarget.shape)\n",
    "print(lvtarget.shape)\n",
    "print(wtarget.shape)\n",
    "print(wdata.shape)\n",
    "print(vdata.shape)\n",
    "print(data.shape)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE is a technique for making imbalanced data more trainable for machine learning. It clusters data and creates a linear model that creates new data that 'looks' like the real dat by randomly creating new data points along each linear connection between points.\n",
    "\n",
    "Here are the docs, Check it out!\n",
    "\n",
    "https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only creating new samples up to 50% of the majority class.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(sampling_strategy=0.5, random_state=2)\n",
    "\n",
    "ccd_SMOTE, cct_SMOTE = sm.fit_resample(ccd,cct)\n",
    "\n",
    "ccd_SMOTE = pd.DataFrame(ccd_SMOTE, columns=ccd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "285715"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see how many new minority samples there are.\n",
    "print(len(cct))\n",
    "len(cct_SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this notebook we are going to go over some of the most common basic machine learning models for classification. The list will be Lasso, Logistic Regression, SVM, Decision Trees, Random Forest, KNeighbor, Multinomial NB, and One vs Rest. This should be a good coverage of models for most use cases. We won't apply all of them for every dataset as that would take a long time and this would turn into a MASSIVE notebook.\n",
    "\n",
    "Here is the docs for SKLearn. It is the goto for machine learning as it has basically every model and package you need to do it!\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html\n",
    "\n",
    "We will be using K-folds for the validation of each model as that is the standard. Feel free to change the number of folds for each model and play with the hyperparameters for each model's docs which I have linked below!\n",
    "\n",
    "\n",
    "### Model Docs:\n",
    "\n",
    "'''\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "I am going to set the hyperparameters by hand. You can automate this process and create a better model using any of the following methods below! These are super helpful and can get you out of tight spots if you can't figure out the proper model hyperparameters!\n",
    "\n",
    "https://autonomio.github.io/docs_talos/#introduction\n",
    "\n",
    "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a\n",
    "\n",
    "https://medium.com/@mikkokotila/a-comprehensive-list-of-hyperparameter-optimization-tuning-solutions-88e067f19d9\n",
    "\n",
    "https://tsfresh.readthedocs.io/en/latest/\n",
    "\n",
    "https://towardsdatascience.com/machine-learning-introduction-a-comprehensive-guide-af6712cf68a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.9666666666666667\n",
      "Fold: 1 score: 0.9333333333333333\n",
      "Fold: 2 score: 0.9666666666666667\n",
      "Fold: 3 score: 0.9\n",
      "Fold: 4 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# First we start with The classic. Logistic Regression! Lets compare this model on Iris and Wine.\n",
    "\n",
    "x = irisdata\n",
    "y = iristarget\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "# This will be how we set up our models. This will be VERY repetitive.\n",
    "# In SKLearn every model can be changed by just changing names.\n",
    "# So that is what we will do!\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    # This is where we define our model.\n",
    "    model = LogisticRegression(n_jobs=-1)\n",
    "    # This is what trains our model\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    # And this is what tells us how good the model is doing on each fold.\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "# We will use this set up for everything going forward.\n",
    "# This will be a little boring and repetitive but that's machine learning for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the model is trained very easily and gets a perfect accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.7084615384615385\n",
      "Fold: 1 score: 0.7423076923076923\n",
      "Fold: 2 score: 0.7313317936874519\n",
      "Fold: 3 score: 0.7474980754426482\n",
      "Fold: 4 score: 0.7551963048498845\n"
     ]
    }
   ],
   "source": [
    "# Now again for the wine dataset.\n",
    "\n",
    "x = winedata\n",
    "y = winetarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = LogisticRegression(n_jobs=-1)\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.7369590809498432\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay lets use these results as a standard for wine and iris to beat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.9016287285670783\n",
      "Fold: 1 score: 0.8944459740452599\n",
      "Fold: 2 score: 0.9126333960814434\n",
      "Fold: 3 score: 0.8644030348019343\n",
      "Fold: 4 score: 0.9014665263170626\n"
     ]
    }
   ],
   "source": [
    "# Next lets do some Lasso! Lets compare this model on Iris and Wine.\n",
    "\n",
    "x = irisdata\n",
    "y = iristarget\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = Lasso(alpha=0.1)\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.8949155319625557\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.22745917659164092\n",
      "Fold: 1 score: 0.23536508597590403\n",
      "Fold: 2 score: 0.23135618090583232\n",
      "Fold: 3 score: 0.2547914280732382\n",
      "Fold: 4 score: 0.2735485469792579\n"
     ]
    }
   ],
   "source": [
    "# Now the wine\n",
    "\n",
    "x = winedata\n",
    "y = winetarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = Lasso(alpha=.001)\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.24450408370517468\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Lasso works almost as well for iris but CONSIDERABLY worse for wine. It really needs L2 on top of L1 to make the model work well.\n",
    "\n",
    "Onto the next model! We will do Decision Trees on both of them to see if we can finally crack that wine dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 1.0\n",
      "Fold: 1 score: 0.9333333333333333\n",
      "Fold: 2 score: 0.9666666666666667\n",
      "Fold: 3 score: 0.8666666666666667\n",
      "Fold: 4 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# First Iris\n",
    "\n",
    "x = irisdata\n",
    "y = iristarget\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.7623076923076924\n",
      "Fold: 1 score: 0.7823076923076923\n",
      "Fold: 2 score: 0.7782909930715936\n",
      "Fold: 3 score: 0.7590454195535027\n",
      "Fold: 4 score: 0.7875288683602771\n"
     ]
    }
   ],
   "source": [
    "# Now lest see if wine does any better\n",
    "\n",
    "x = winedata\n",
    "y = winetarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.7738961331201516\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope! Hmmm Well maybe one of the other models will do better!\n",
    "\n",
    "Lets try SVM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 1.0\n",
      "Fold: 1 score: 0.9666666666666667\n",
      "Fold: 2 score: 1.0\n",
      "Fold: 3 score: 0.8666666666666667\n",
      "Fold: 4 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# First iris because of my ocd\n",
    "\n",
    "x = irisdata\n",
    "y = iristarget\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = LinearSVC()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same old same old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.4576923076923077\n",
      "Fold: 1 score: 0.6253846153846154\n",
      "Fold: 2 score: 0.6297151655119323\n",
      "Fold: 3 score: 0.42340261739799845\n",
      "Fold: 4 score: 0.6943802925327175\n"
     ]
    }
   ],
   "source": [
    "# Moment of truth!\n",
    "\n",
    "x = winedata\n",
    "y = winetarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = LinearSVC()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.5661149997039142\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOPE! Well lets try another. But first lets see how our ICU data does with SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.2689660936672889\n",
      "Fold: 1 score: 0.264543630892678\n",
      "Fold: 2 score: 0.1756822646030397\n",
      "Fold: 3 score: 0.23882364413981416\n",
      "Fold: 4 score: 0.15472707514837417\n"
     ]
    }
   ],
   "source": [
    "# This will take a LONG time!\n",
    "\n",
    "x = data\n",
    "y = ltarget\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = LinearSVC()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was awful. It was almost as bad as random guessing!\n",
    "\n",
    "We will try the ICU data later on with two more models to see if we can get it better. Next lets try wine with a RandomForest and hope that we get higher then .75!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.8015384615384615\n",
      "Fold: 1 score: 0.816923076923077\n",
      "Fold: 2 score: 0.8052347959969207\n",
      "Fold: 3 score: 0.8244803695150116\n",
      "Fold: 4 score: 0.7998460354118553\n"
     ]
    }
   ],
   "source": [
    "x = winedata\n",
    "y = winetarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.8096045478770652\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AN IMPROVEMENT!!! Yay, Randomforest is our best bet so far for wine. This means XGBoost will probably be the best for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.999420666409185\n",
      "Fold: 1 score: 0.9994908886626171\n",
      "Fold: 2 score: 0.9996313266972139\n",
      "Fold: 3 score: 0.9994557679816014\n",
      "Fold: 4 score: 0.9996137708256526\n"
     ]
    }
   ],
   "source": [
    "# Lets try it on the CCD for both regular and SMOTE data. \n",
    "\n",
    "x = ccdata\n",
    "y = cctarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.999522484115254\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.999597508049839\n",
      "Fold: 1 score: 0.999615007699846\n",
      "Fold: 2 score: 0.999527501181247\n",
      "Fold: 3 score: 0.9996499947499212\n",
      "Fold: 4 score: 0.9998074971124566\n"
     ]
    }
   ],
   "source": [
    "# This is the SMOTE data we made earlier.\n",
    "x = ccd_SMOTE\n",
    "y = cct_SMOTE\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y)):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.999639501758662\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! This is an amazing result! Again RandomForest to the resuce! Remember we added 90,000+ samples to the SMOTE data. So we should expect to see a decrease if the model stayed the same as it will predict the minority class at the same rate and fail more. But this model actually increased! Almost leading to a perfect score! XGBoost should gives us the best model possible from these results.\n",
    "\n",
    "Now lets try K Nearest Neighbors to see if that will work on wine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 1.0\n",
      "Fold: 1 score: 0.9666666666666667\n",
      "Fold: 2 score: 1.0\n",
      "Fold: 3 score: 0.8666666666666667\n",
      "Fold: 4 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Again Iris first :)\n",
    "\n",
    "x = irisdata\n",
    "y = iristarget\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.6646153846153846\n",
      "Fold: 1 score: 0.6761538461538461\n",
      "Fold: 2 score: 0.6913010007698229\n",
      "Fold: 3 score: 0.6820631254811393\n",
      "Fold: 4 score: 0.6682063125481139\n"
     ]
    }
   ],
   "source": [
    "# WORK FOR ME!\n",
    "\n",
    "x = winedata\n",
    "y = winetarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.6764679339136614\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":( Even worse. Looks like RandomForest will be the best wiht only 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.9994733330992591\n",
      "Fold: 1 score: 0.9994908886626171\n",
      "Fold: 2 score: 0.9995962149540913\n",
      "Fold: 3 score: 0.9995435473394076\n",
      "Fold: 4 score: 0.9994733238531627\n"
     ]
    }
   ],
   "source": [
    "# We will also try it for our ccd and smote ccd\n",
    "\n",
    "x = ccdata\n",
    "y = cctarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.9995154615817077\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.998302533949321\n",
      "Fold: 1 score: 0.998792524149517\n",
      "Fold: 2 score: 0.9987050032374919\n",
      "Fold: 3 score: 0.998599978999685\n",
      "Fold: 4 score: 0.9986874803122047\n"
     ]
    }
   ],
   "source": [
    "x = ccd_SMOTE\n",
    "y = cct_SMOTE\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y)):\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.9986175041296439\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly the non smote data does better then expected and the smote data does much better too if you consider the amount of new data being added is 90,000 samples! So a 99.8% result is pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.9988413328183702\n",
      "Fold: 1 score: 0.9986131104947158\n",
      "Fold: 2 score: 0.9989290918347641\n",
      "Fold: 3 score: 0.9988764242200804\n",
      "Fold: 4 score: 0.9989290918347641\n"
     ]
    }
   ],
   "source": [
    "# We will also try these methods for our big datasets to see if we get any improvement\n",
    "# as they can deal with large data better then the others.\n",
    "\n",
    "x = abs(ccdata)\n",
    "y = cctarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.998837810240539\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.9403786924261515\n",
      "Fold: 1 score: 0.9403611927761445\n",
      "Fold: 2 score: 0.9406926482683793\n",
      "Fold: 3 score: 0.9400091001365021\n",
      "Fold: 4 score: 0.9407091106366595\n"
     ]
    }
   ],
   "source": [
    "# Again for SMOTE\n",
    "\n",
    "x = abs(ccd_SMOTE)\n",
    "y = cct_SMOTE\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y)):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.9404301488487674\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like KNN is better than Naive Bayes for this Problem, Will the ICU do any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.2706237985122447\n",
      "Fold: 1 score: 0.26834670678702777\n",
      "Fold: 2 score: 0.2708440717161445\n",
      "Fold: 3 score: 0.2681350218024268\n",
      "Fold: 4 score: 0.26852238847557747\n"
     ]
    }
   ],
   "source": [
    "# Now our BIG big boy\n",
    "\n",
    "x = abs(data)\n",
    "y = ltarget\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.2692943974586842\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, Still stuck at a really low value. It looks like we'll need a much more powerful model to get good results on this.\n",
    "\n",
    "This shows the limits of classical machine learning fairly well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 1.0\n",
      "Fold: 1 score: 0.9333333333333333\n",
      "Fold: 2 score: 1.0\n",
      "Fold: 3 score: 0.9\n",
      "Fold: 4 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Now lets end this with a 1 v rest method.\n",
    "# It creates a seperate classifier for each class. So lets run it on our multiclass problems\n",
    "\n",
    "x = irisdata\n",
    "y = iristarget\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = OneVsRestClassifier(estimator=RandomForestClassifier(), n_jobs=-1)\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 score: 0.9943303708243947\n",
      "Fold: 1 score: 0.9939680151565808\n",
      "Fold: 2 score: 0.9945669587506791\n",
      "Fold: 3 score: 0.9935637564257951\n",
      "Fold: 4 score: 0.9944969210621639\n"
     ]
    }
   ],
   "source": [
    "# LAST ONE!\n",
    "\n",
    "x = data\n",
    "y = ltarget\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = OneVsRestClassifier(estimator=RandomForestClassifier(), n_jobs=-1)\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy RF 0.9941852044439228\n"
     ]
    }
   ],
   "source": [
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last one is amazing results! Maaaaayyyybbbbeeee too amazing. We could use our validation data to test for overfitting but Its safe to assume that it is overfitting as our best model in the competition only got 85%!!! To fix this we could fine tune the model or use a different classifier. Maybe the highest working binary classifier for large, complex data?\n",
    "\n",
    "That will be the last test we do for this notebook, To see the Deep learning or XGBoost/LightGBM notebook check those out!\n",
    "\n",
    "Thanks for reading :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
